---

title: NN Building Block


keywords: fastai
sidebar: home_sidebar



nb_path: "nbdev_nbs/model/building_block.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbdev_nbs/model/building_block.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Description">Description<a class="anchor-link" href="#Description"> </a></h2><p>The building block module specifies the architectures of the core neural networks used in PeptDeep.</p>
<p>All networks are based on the <a href="&#39;https://pytorch.org/&#39;">PyTorch</a> package by subclassing <code>torch.nn.Module</code>, which is the base class for all neural networks. To implement the Transformer-network, the HuggingFace <a href="&#39;https://huggingface.co/docs/transformers/&#39;">transformers</a> package is used, which allows to specify transformer networks in Pytorch.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Variables">Variables<a class="anchor-link" href="#Variables"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Utility-Functions">Utility Functions<a class="anchor-link" href="#Utility-Functions"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Embeddings">Embeddings<a class="anchor-link" href="#Embeddings"> </a></h3><p>Basic embeddings or encodings of inputs such as AA sequence or modification state</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="aa_embedding" class="doc_header"><code>aa_embedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L35" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>aa_embedding</code>(<strong><code>hidden_size</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ascii_embedding" class="doc_header"><code>ascii_embedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L38" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ascii_embedding</code>(<strong><code>hidden_size</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="aa_one_hot" class="doc_header"><code>aa_one_hot</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L41" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>aa_one_hot</code>(<strong><code>aa_indices</code></strong>, <strong>*<code>cat_others</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instrument_embedding" class="doc_header"><code>instrument_embedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L47" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instrument_embedding</code>(<strong><code>hidden_size</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Initial-states">Initial states<a class="anchor-link" href="#Initial-states"> </a></h3><p>Generates tensors defining the initial (hidden) states of the elements in the input sequence</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="zero_param" class="doc_header"><code>zero_param</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L51" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>zero_param</code>(<strong>*<code>shape</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="xavier_param" class="doc_header"><code>xavier_param</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L54" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>xavier_param</code>(<strong>*<code>shape</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Networks">Networks<a class="anchor-link" href="#Networks"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Networks-for-sequences">Networks for sequences<a class="anchor-link" href="#Networks-for-sequences"> </a></h3><p>The seq networks take the sequence and possible accompanying information such as charge state or modification and apply neural network transformations.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqCNN_MultiKernel" class="doc_header"><code>class</code> <code>SeqCNN_MultiKernel</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L62" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqCNN_MultiKernel</code>(<strong><code>out_features</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<p>Extract sequence features using <code>torch.nn.Conv1D</code> with
different kernel sizes (1(residue connection),3,5,7),
and then concatenate the outputs of these Conv1Ds.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqCNN" class="doc_header"><code>class</code> <code>SeqCNN</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L103" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqCNN</code>(<strong><code>embedding_hidden</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Extract sequence features using <code>torch.nn.Conv1D</code> with
different kernel sizes (1(residue connection),3,5,7), and then concatenate
the outputs of these Conv1Ds. The Output dim is 4*embedding_hidden.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="PyTorch-Built-in-Transformers-(for-test-only)">PyTorch Built-in Transformers (for test only)<a class="anchor-link" href="#PyTorch-Built-in-Transformers-(for-test-only)"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq_Transformer" class="doc_header"><code>class</code> <code>Seq_Transformer</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L135" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq_Transformer</code>(<strong><code>in_features</code></strong>, <strong><code>hidden_features</code></strong>, <strong><code>nheads</code></strong>=<em><code>8</code></em>, <strong><code>nlayers</code></strong>=<em><code>2</code></em>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Using PyTorch built-in Transformer layers</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Hidden_Transformer" class="doc_header"><code>class</code> <code>Hidden_Transformer</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L158" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Hidden_Transformer</code>(<strong><code>hidden</code></strong>, <strong><code>hidden_expand</code></strong>=<em><code>4</code></em>, <strong><code>nheads</code></strong>=<em><code>8</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Transformer NN based on pytorch's built-in TransformerLayer class</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="HuggingFace-Transformers">HuggingFace Transformers<a class="anchor-link" href="#HuggingFace-Transformers"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Hidden_HFace_Transformer" class="doc_header"><code>class</code> <code>Hidden_HFace_Transformer</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L204" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Hidden_HFace_Transformer</code>(<strong><code>hidden_dim</code></strong>, <strong><code>hidden_expand</code></strong>=<em><code>4</code></em>, <strong><code>nheads</code></strong>=<em><code>8</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>, <strong><code>output_attentions</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Transformer NN based on HuggingFace's BertEncoder class</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="HFace_Transformer_with_PositionalEncoder" class="doc_header"><code>class</code> <code>HFace_Transformer_with_PositionalEncoder</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L239" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>HFace_Transformer_with_PositionalEncoder</code>(<strong><code>hidden_dim</code></strong>:<code>int</code>, <strong><code>hidden_expand</code></strong>=<em><code>4</code></em>, <strong><code>nheads</code></strong>=<em><code>8</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>, <strong><code>output_attentions</code></strong>=<em><code>False</code></em>, <strong><code>max_len</code></strong>=<em><code>200</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqLSTM" class="doc_header"><code>class</code> <code>SeqLSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L280" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqLSTM</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>, <strong><code>rnn_layer</code></strong>=<em><code>2</code></em>, <strong><code>bidirectional</code></strong>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqGRU" class="doc_header"><code>class</code> <code>SeqGRU</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L319" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqGRU</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>, <strong><code>rnn_layer</code></strong>=<em><code>2</code></em>, <strong><code>bidirectional</code></strong>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Linear-Seq-Transformations">Linear Seq Transformations<a class="anchor-link" href="#Linear-Seq-Transformations"> </a></h4><p>Takes in a sequence and applies a linear transformation on it</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqAttentionSum" class="doc_header"><code>class</code> <code>SeqAttentionSum</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L355" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqAttentionSum</code>(<strong><code>in_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>apply linear transformation and tensor rescaling with softmax</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Positional-Encoding-and-Embedding">Positional Encoding and Embedding<a class="anchor-link" href="#Positional-Encoding-and-Embedding"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PositionalEncoding" class="doc_header"><code>class</code> <code>PositionalEncoding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L371" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PositionalEncoding</code>(<strong><code>out_features</code></strong>=<em><code>128</code></em>, <strong><code>max_len</code></strong>=<em><code>200</code></em>) :: <code>Module</code></p>
</blockquote>
<p>transform sequence input into a positional representation</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PositionalEmbedding" class="doc_header"><code>class</code> <code>PositionalEmbedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L392" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PositionalEmbedding</code>(<strong><code>out_features</code></strong>=<em><code>128</code></em>, <strong><code>max_len</code></strong>=<em><code>200</code></em>) :: <code>Module</code></p>
</blockquote>
<p>transform sequence with the standard embedding function</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Input-Networks">Input Networks<a class="anchor-link" href="#Input-Networks"> </a></h3><p>The 'Input' classes represent the input layers of the networks, meaning they interact directly with the (formatted) features such as the peptide sequence, the charge state, the modifications or the collision energy</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Linear-input-transformations-and-embeddings.">Linear input transformations and embeddings.<a class="anchor-link" href="#Linear-input-transformations-and-embeddings."> </a></h4><p>Performing embedding and linear operations on the input</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Meta_Embedding" class="doc_header"><code>class</code> <code>Meta_Embedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L409" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Meta_Embedding</code>(<strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Encodes Charge state, Normalized Collision Energy (NCE) and Instrument for a given spectrum
into a 'meta' single layer network</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Mod_Embedding_FixFirstK" class="doc_header"><code>class</code> <code>Mod_Embedding_FixFirstK</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L434" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Mod_Embedding_FixFirstK</code>(<strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Encodes the modification vector in a single layer feed forward network, but not transforming the first k features</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AA_Mod_Embedding" class="doc_header"><code>class</code> <code>AA_Mod_Embedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L458" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AA_Mod_Embedding</code>(<strong><code>out_features</code></strong>, <strong><code>mod_feature_size</code></strong>=<em><code>8</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Concatenates the AA (128 ASCII codes) embedding with the modifcation vector</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Mod_Embedding" class="doc_header"><code>class</code> <code>Mod_Embedding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L480" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Mod_Embedding</code>(<strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Encodes the modification vector in a single layer feed forward network</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Input_26AA_Mod_PositionalEncoding" class="doc_header"><code>class</code> <code>Input_26AA_Mod_PositionalEncoding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L500" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Input_26AA_Mod_PositionalEncoding</code>(<strong><code>out_features</code></strong>, <strong><code>max_len</code></strong>=<em><code>200</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encodes AA (26 AA letters) and modification vector</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Input_AA_Mod_PositionalEncoding" class="doc_header"><code>class</code> <code>Input_AA_Mod_PositionalEncoding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L524" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Input_AA_Mod_PositionalEncoding</code>(<strong><code>out_features</code></strong>, <strong><code>max_len</code></strong>=<em><code>200</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encodes AA (ASCII codes) and modification vector</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Input_AA_Mod_Charge_PositionalEncoding" class="doc_header"><code>class</code> <code>Input_AA_Mod_Charge_PositionalEncoding</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L546" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Input_AA_Mod_Charge_PositionalEncoding</code>(<strong><code>out_features</code></strong>, <strong><code>max_len</code></strong>=<em><code>200</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Embed AA (128 ASCII codes), modification, and charge state</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="LSTM">LSTM<a class="anchor-link" href="#LSTM"> </a></h4><p>Applying LSTMs to the input</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Input_26AA_Mod_LSTM" class="doc_header"><code>class</code> <code>Input_26AA_Mod_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L574" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Input_26AA_Mod_LSTM</code>(<strong><code>out_features</code></strong>, <strong><code>n_lstm_layers</code></strong>=<em><code>1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Applies an LSTM network to a AA (26 AA letters) sequence &amp; modifications</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Input_26AA_Mod_Meta_LSTM" class="doc_header"><code>class</code> <code>Input_26AA_Mod_Meta_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L599" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Input_26AA_Mod_Meta_LSTM</code>(<strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Applies a LSTM network to a AA (26 AA letters) sequence and modifications,
and concatenates with 'meta' information (charge, nce, instrument_indices)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Input_26AA_Mod_Charge_LSTM" class="doc_header"><code>class</code> <code>Input_26AA_Mod_Charge_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L632" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Input_26AA_Mod_Charge_LSTM</code>(<strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Applies a LSTM network to a AA (26 AA letters) sequence and modifications,
and concatenates with charge state information</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Complex-Seq-Layers-(or-Output-Layers)">Complex Seq Layers (or Output Layers)<a class="anchor-link" href="#Complex-Seq-Layers-(or-Output-Layers)"> </a></h3><p>The 'Output' classes represent the output layers of the networks, meaning they take the hidden layer of a network as input, transform it into the output such as a spectrum, ccs value, rt</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq_Meta_LSTM" class="doc_header"><code>class</code> <code>Seq_Meta_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L663" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq_Meta_LSTM</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>takes a hidden layer which processes the hidden tensor
as well as the 'meta' information of NCE, Instrument, Charge</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq_Meta_Linear" class="doc_header"><code>class</code> <code>Seq_Meta_Linear</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L689" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq_Meta_Linear</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>takes a hidden linear which processed the 'meta' information of NCE, Instrument, Charge</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoders">Encoders<a class="anchor-link" href="#Encoders"> </a></h3><p>The encoder classes transform the features into a learned representation. Within the encoder, the <code>Input..</code> classes are used to transform the features into a network representation. Subsequently, Convolutional Neural Networks (CNNs) and/or Long Short-Term Memory (LSTM) Networks and/or Linear transformations are applied to the data.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_26AA_Mod_LSTM" class="doc_header"><code>class</code> <code>Encoder_26AA_Mod_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L716" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_26AA_Mod_LSTM</code>(<strong><code>out_features</code></strong>, <strong><code>n_lstm_layers</code></strong>=<em><code>1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Two LSTM layers on AA (26 AA letters) and modifications.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_26AA_Mod_CNN_LSTM" class="doc_header"><code>class</code> <code>Encoder_26AA_Mod_CNN_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L738" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_26AA_Mod_CNN_LSTM</code>(<strong><code>out_features</code></strong>, <strong><code>n_lstm_layers</code></strong>=<em><code>1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (26 AA letters) and modifications by CNN and LSTM layers</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_26AA_Mod_CNN_LSTM_AttnSum" class="doc_header"><code>class</code> <code>Encoder_26AA_Mod_CNN_LSTM_AttnSum</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L764" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_26AA_Mod_CNN_LSTM_AttnSum</code>(<strong><code>out_features</code></strong>, <strong><code>n_lstm_layers</code></strong>=<em><code>2</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (26 AA letters) and modifications by CNN and LSTM layers,
then by 'SeqAttentionSum'.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_AA_Mod_CNN_LSTM_AttnSum" class="doc_header"><code>class</code> <code>Encoder_AA_Mod_CNN_LSTM_AttnSum</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L793" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_AA_Mod_CNN_LSTM_AttnSum</code>(<strong><code>out_features</code></strong>, <strong><code>n_lstm_layers</code></strong>=<em><code>2</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (128 ASCII codes) and modifications by CNN and LSTM layers,
and then by 'SeqAttentionSum'.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_AA_Mod_Transformer" class="doc_header"><code>class</code> <code>Encoder_AA_Mod_Transformer</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L821" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_AA_Mod_Transformer</code>(<strong><code>out_features</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>output_attentions</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>AAs (128 ASCII codes) and modifications embedded by CNN and LSTM layers,
then encoded by 'SeqAttentionSum'.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_AA_Mod_Transformer_AttnSum" class="doc_header"><code>class</code> <code>Encoder_AA_Mod_Transformer_AttnSum</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L854" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_AA_Mod_Transformer_AttnSum</code>(<strong><code>out_features</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>output_attentions</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (128 ASCII codes) and modifications by transformers.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_AA_Mod_Charge_Transformer" class="doc_header"><code>class</code> <code>Encoder_AA_Mod_Charge_Transformer</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L877" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_AA_Mod_Charge_Transformer</code>(<strong><code>out_features</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>output_attentions</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (128 ASCII codes), modifications and charge by transformers.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_AA_Mod_Charge_Transformer_AttnSum" class="doc_header"><code>class</code> <code>Encoder_AA_Mod_Charge_Transformer_AttnSum</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L908" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_AA_Mod_Charge_Transformer_AttnSum</code>(<strong><code>out_features</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>, <strong><code>nlayers</code></strong>=<em><code>4</code></em>, <strong><code>output_attentions</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (128 ASCII codes), modifications and charge by transformers,
and then by 'SeqAttentionSum'</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum" class="doc_header"><code>class</code> <code>Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L931" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum</code>(<strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Encode AAs (26 AA letters), modifications and charge by transformers,
and then by 'SeqAttentionSum'</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoders">Decoders<a class="anchor-link" href="#Decoders"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Decoder_LSTM" class="doc_header"><code>class</code> <code>Decoder_LSTM</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L965" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Decoder_LSTM</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Decode with LSTM</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Decoder_GRU" class="doc_header"><code>class</code> <code>Decoder_GRU</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L991" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Decoder_GRU</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Decode with GRU</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Decoder_Linear" class="doc_header"><code>class</code> <code>Decoder_Linear</code><a href="https://github.com/MannLabs/peptdeep/tree/main/peptdeep/model/building_block.py#L1018" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Decoder_Linear</code>(<strong><code>in_features</code></strong>, <strong><code>out_features</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Decode w linear NN</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

