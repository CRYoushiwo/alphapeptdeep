<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>alphapeptdeep - NN Building Block</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../peptdeep.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="alphapeptdeep - NN Building Block">
<meta property="og:description" content="The building block module specifies the architectures of the core neural networks used in PeptDeep.">
<meta property="og:site-name" content="alphapeptdeep">
<meta name="twitter:title" content="alphapeptdeep - NN Building Block">
<meta name="twitter:description" content="The building block module specifies the architectures of the core neural networks used in PeptDeep.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">alphapeptdeep</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/MannLabs/alphapeptdeep"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">NN Building Block</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">AlphaPeptDeep</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pipeline_api.html" class="sidebar-item-text sidebar-link">pipeline API</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pretrained_models.html" class="sidebar-item-text sidebar-link">Pretrained Models</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../settings.html" class="sidebar-item-text sidebar-link">Settings</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../test_ui.html" class="sidebar-item-text sidebar-link">Testing GUI</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../utils.html" class="sidebar-item-text sidebar-link">Utils</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">mass_spec</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mass_spec/mass_calibration.html" class="sidebar-item-text sidebar-link">Mass calibration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mass_spec/match.html" class="sidebar-item-text sidebar-link">Match</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mass_spec/ms_reader.html" class="sidebar-item-text sidebar-link">MS Reader</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">model</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/building_block.html" class="sidebar-item-text sidebar-link active">NN Building Block</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/ccs.html" class="sidebar-item-text sidebar-link">CCS model and interface</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/featurize.html" class="sidebar-item-text sidebar-link">Featurize</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/generic_property_prediction.html" class="sidebar-item-text sidebar-link">Generic Property Prediction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model_interface.html" class="sidebar-item-text sidebar-link">Model Interface</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model_shop.html" class="sidebar-item-text sidebar-link">Model shop</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/ms2.html" class="sidebar-item-text sidebar-link">MS2 prediction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/rt.html" class="sidebar-item-text sidebar-link">Retention Time Prediction</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">protein</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../protein/fasta.html" class="sidebar-item-text sidebar-link">Spectral Library from fasta</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">psm_frag_reader</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/library_frag_reader.html" class="sidebar-item-text sidebar-link">TSV Fragment Reader</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/maxquant_frag_reader.html" class="sidebar-item-text sidebar-link">MaxQuant Fragment Reader</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/psm_frag_reader.html" class="sidebar-item-text sidebar-link">Base PSM Fragment Reader</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/psmlabel_reader.html" class="sidebar-item-text sidebar-link">.psmlabel Fragment Reader</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">rescore</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rescore/fdr.html" class="sidebar-item-text sidebar-link">FDR</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rescore/feature_extractor.html" class="sidebar-item-text sidebar-link">Feature Extractor</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rescore/percolator.html" class="sidebar-item-text sidebar-link">Percolator</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">spec_lib</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/library_factory.html" class="sidebar-item-text sidebar-link">Library Factory</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/predict_lib.html" class="sidebar-item-text sidebar-link">Predict Spectral Library</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/test_translate_tsv.html" class="sidebar-item-text sidebar-link">test_translate_tsv.html</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/translate.html" class="sidebar-item-text sidebar-link">Translate Spectral Library</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#description" id="toc-description" class="nav-link active" data-scroll-target="#description">Description</a></li>
  <li><a href="#variables" id="toc-variables" class="nav-link" data-scroll-target="#variables">Variables</a></li>
  <li><a href="#utility-functions" id="toc-utility-functions" class="nav-link" data-scroll-target="#utility-functions">Utility Functions</a>
  <ul class="collapse">
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a></li>
  <li><a href="#instrument_embedding" id="toc-instrument_embedding" class="nav-link" data-scroll-target="#instrument_embedding">instrument_embedding</a></li>
  <li><a href="#aa_one_hot" id="toc-aa_one_hot" class="nav-link" data-scroll-target="#aa_one_hot">aa_one_hot</a></li>
  <li><a href="#ascii_embedding" id="toc-ascii_embedding" class="nav-link" data-scroll-target="#ascii_embedding">ascii_embedding</a></li>
  <li><a href="#aa_embedding" id="toc-aa_embedding" class="nav-link" data-scroll-target="#aa_embedding">aa_embedding</a></li>
  <li><a href="#initial-states" id="toc-initial-states" class="nav-link" data-scroll-target="#initial-states">Initial states</a></li>
  <li><a href="#xavier_param" id="toc-xavier_param" class="nav-link" data-scroll-target="#xavier_param">xavier_param</a></li>
  <li><a href="#zero_param" id="toc-zero_param" class="nav-link" data-scroll-target="#zero_param">zero_param</a></li>
  </ul></li>
  <li><a href="#networks" id="toc-networks" class="nav-link" data-scroll-target="#networks">Networks</a>
  <ul class="collapse">
  <li><a href="#networks-for-sequences" id="toc-networks-for-sequences" class="nav-link" data-scroll-target="#networks-for-sequences">Networks for sequences</a></li>
  <li><a href="#seqcnn" id="toc-seqcnn" class="nav-link" data-scroll-target="#seqcnn">SeqCNN</a></li>
  <li><a href="#seqcnn_multikernel" id="toc-seqcnn_multikernel" class="nav-link" data-scroll-target="#seqcnn_multikernel">SeqCNN_MultiKernel</a></li>
  <li><a href="#hidden_transformer" id="toc-hidden_transformer" class="nav-link" data-scroll-target="#hidden_transformer">Hidden_Transformer</a></li>
  <li><a href="#seq_transformer" id="toc-seq_transformer" class="nav-link" data-scroll-target="#seq_transformer">Seq_Transformer</a></li>
  <li><a href="#hface_transformer_with_positionalencoder" id="toc-hface_transformer_with_positionalencoder" class="nav-link" data-scroll-target="#hface_transformer_with_positionalencoder">HFace_Transformer_with_PositionalEncoder</a></li>
  <li><a href="#hidden_hface_transformer" id="toc-hidden_hface_transformer" class="nav-link" data-scroll-target="#hidden_hface_transformer">Hidden_HFace_Transformer</a></li>
  <li><a href="#seqlstm" id="toc-seqlstm" class="nav-link" data-scroll-target="#seqlstm">SeqLSTM</a></li>
  <li><a href="#seqgru" id="toc-seqgru" class="nav-link" data-scroll-target="#seqgru">SeqGRU</a></li>
  <li><a href="#seqattentionsum" id="toc-seqattentionsum" class="nav-link" data-scroll-target="#seqattentionsum">SeqAttentionSum</a></li>
  <li><a href="#positionalembedding" id="toc-positionalembedding" class="nav-link" data-scroll-target="#positionalembedding">PositionalEmbedding</a></li>
  <li><a href="#positionalencoding" id="toc-positionalencoding" class="nav-link" data-scroll-target="#positionalencoding">PositionalEncoding</a></li>
  <li><a href="#input-networks" id="toc-input-networks" class="nav-link" data-scroll-target="#input-networks">Input Networks</a></li>
  <li><a href="#input_aa_mod_charge_positionalencoding" id="toc-input_aa_mod_charge_positionalencoding" class="nav-link" data-scroll-target="#input_aa_mod_charge_positionalencoding">Input_AA_Mod_Charge_PositionalEncoding</a></li>
  <li><a href="#input_aa_mod_positionalencoding" id="toc-input_aa_mod_positionalencoding" class="nav-link" data-scroll-target="#input_aa_mod_positionalencoding">Input_AA_Mod_PositionalEncoding</a></li>
  <li><a href="#input_26aa_mod_positionalencoding" id="toc-input_26aa_mod_positionalencoding" class="nav-link" data-scroll-target="#input_26aa_mod_positionalencoding">Input_26AA_Mod_PositionalEncoding</a></li>
  <li><a href="#mod_embedding" id="toc-mod_embedding" class="nav-link" data-scroll-target="#mod_embedding">Mod_Embedding</a></li>
  <li><a href="#aa_mod_embedding" id="toc-aa_mod_embedding" class="nav-link" data-scroll-target="#aa_mod_embedding">AA_Mod_Embedding</a></li>
  <li><a href="#mod_embedding_fixfirstk" id="toc-mod_embedding_fixfirstk" class="nav-link" data-scroll-target="#mod_embedding_fixfirstk">Mod_Embedding_FixFirstK</a></li>
  <li><a href="#meta_embedding" id="toc-meta_embedding" class="nav-link" data-scroll-target="#meta_embedding">Meta_Embedding</a></li>
  </ul></li>
  <li><a href="#lstm" id="toc-lstm" class="nav-link" data-scroll-target="#lstm">LSTM</a>
  <ul class="collapse">
  <li><a href="#input_26aa_mod_charge_lstm" id="toc-input_26aa_mod_charge_lstm" class="nav-link" data-scroll-target="#input_26aa_mod_charge_lstm">Input_26AA_Mod_Charge_LSTM</a></li>
  <li><a href="#input_26aa_mod_meta_lstm" id="toc-input_26aa_mod_meta_lstm" class="nav-link" data-scroll-target="#input_26aa_mod_meta_lstm">Input_26AA_Mod_Meta_LSTM</a></li>
  <li><a href="#input_26aa_mod_lstm" id="toc-input_26aa_mod_lstm" class="nav-link" data-scroll-target="#input_26aa_mod_lstm">Input_26AA_Mod_LSTM</a></li>
  <li><a href="#complex-seq-layers-or-output-layers" id="toc-complex-seq-layers-or-output-layers" class="nav-link" data-scroll-target="#complex-seq-layers-or-output-layers">Complex Seq Layers (or Output Layers)</a></li>
  <li><a href="#seq_meta_linear" id="toc-seq_meta_linear" class="nav-link" data-scroll-target="#seq_meta_linear">Seq_Meta_Linear</a></li>
  <li><a href="#seq_meta_lstm" id="toc-seq_meta_lstm" class="nav-link" data-scroll-target="#seq_meta_lstm">Seq_Meta_LSTM</a></li>
  <li><a href="#encoders" id="toc-encoders" class="nav-link" data-scroll-target="#encoders">Encoders</a></li>
  <li><a href="#encoder_26aa_mod_charge_cnn_lstm_attnsum" id="toc-encoder_26aa_mod_charge_cnn_lstm_attnsum" class="nav-link" data-scroll-target="#encoder_26aa_mod_charge_cnn_lstm_attnsum">Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum</a></li>
  <li><a href="#encoder_aa_mod_charge_transformer_attnsum" id="toc-encoder_aa_mod_charge_transformer_attnsum" class="nav-link" data-scroll-target="#encoder_aa_mod_charge_transformer_attnsum">Encoder_AA_Mod_Charge_Transformer_AttnSum</a></li>
  <li><a href="#encoder_aa_mod_charge_transformer" id="toc-encoder_aa_mod_charge_transformer" class="nav-link" data-scroll-target="#encoder_aa_mod_charge_transformer">Encoder_AA_Mod_Charge_Transformer</a></li>
  <li><a href="#encoder_aa_mod_transformer_attnsum" id="toc-encoder_aa_mod_transformer_attnsum" class="nav-link" data-scroll-target="#encoder_aa_mod_transformer_attnsum">Encoder_AA_Mod_Transformer_AttnSum</a></li>
  <li><a href="#encoder_aa_mod_transformer" id="toc-encoder_aa_mod_transformer" class="nav-link" data-scroll-target="#encoder_aa_mod_transformer">Encoder_AA_Mod_Transformer</a></li>
  <li><a href="#encoder_aa_mod_cnn_lstm_attnsum" id="toc-encoder_aa_mod_cnn_lstm_attnsum" class="nav-link" data-scroll-target="#encoder_aa_mod_cnn_lstm_attnsum">Encoder_AA_Mod_CNN_LSTM_AttnSum</a></li>
  <li><a href="#encoder_26aa_mod_cnn_lstm_attnsum" id="toc-encoder_26aa_mod_cnn_lstm_attnsum" class="nav-link" data-scroll-target="#encoder_26aa_mod_cnn_lstm_attnsum">Encoder_26AA_Mod_CNN_LSTM_AttnSum</a></li>
  <li><a href="#encoder_26aa_mod_cnn_lstm" id="toc-encoder_26aa_mod_cnn_lstm" class="nav-link" data-scroll-target="#encoder_26aa_mod_cnn_lstm">Encoder_26AA_Mod_CNN_LSTM</a></li>
  <li><a href="#encoder_26aa_mod_lstm" id="toc-encoder_26aa_mod_lstm" class="nav-link" data-scroll-target="#encoder_26aa_mod_lstm">Encoder_26AA_Mod_LSTM</a></li>
  <li><a href="#decoders" id="toc-decoders" class="nav-link" data-scroll-target="#decoders">Decoders</a></li>
  <li><a href="#decoder_gru" id="toc-decoder_gru" class="nav-link" data-scroll-target="#decoder_gru">Decoder_GRU</a></li>
  <li><a href="#decoder_lstm" id="toc-decoder_lstm" class="nav-link" data-scroll-target="#decoder_lstm">Decoder_LSTM</a></li>
  <li><a href="#decoder_linear" id="toc-decoder_linear" class="nav-link" data-scroll-target="#decoder_linear">Decoder_Linear</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/MannLabs/alphapeptdeep/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">NN Building Block</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="description" class="level2">
<h2 class="anchored" data-anchor-id="description">Description</h2>
<p>The building block module specifies the architectures of the core neural networks used in PeptDeep.</p>
<p>All networks are based on the <a href="'https://pytorch.org/'">PyTorch</a> package by subclassing <code>torch.nn.Module</code>, which is the base class for all neural networks. To implement the Transformer-network, the HuggingFace <a href="'https://huggingface.co/docs/transformers/'">transformers</a> package is used, which allows to specify transformer networks in Pytorch.</p>
</section>
<section id="variables" class="level2">
<h2 class="anchored" data-anchor-id="variables">Variables</h2>
</section>
<section id="utility-functions" class="level2">
<h2 class="anchored" data-anchor-id="utility-functions">Utility Functions</h2>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>Basic embeddings or encodings of inputs such as AA sequence or modification state</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L53" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="instrument_embedding" class="level3">
<h3 class="anchored" data-anchor-id="instrument_embedding">instrument_embedding</h3>
<blockquote class="blockquote">
<pre><code> instrument_embedding (hidden_size)</code></pre>
</blockquote>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L47" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="aa_one_hot" class="level3">
<h3 class="anchored" data-anchor-id="aa_one_hot">aa_one_hot</h3>
<blockquote class="blockquote">
<pre><code> aa_one_hot (aa_indices, *cat_others)</code></pre>
</blockquote>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L44" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ascii_embedding" class="level3">
<h3 class="anchored" data-anchor-id="ascii_embedding">ascii_embedding</h3>
<blockquote class="blockquote">
<pre><code> ascii_embedding (hidden_size)</code></pre>
</blockquote>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L41" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="aa_embedding" class="level3">
<h3 class="anchored" data-anchor-id="aa_embedding">aa_embedding</h3>
<blockquote class="blockquote">
<pre><code> aa_embedding (hidden_size)</code></pre>
</blockquote>
</section>
<section id="initial-states" class="level3">
<h3 class="anchored" data-anchor-id="initial-states">Initial states</h3>
<p>Generates tensors defining the initial (hidden) states of the elements in the input sequence</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L60" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="xavier_param" class="level3">
<h3 class="anchored" data-anchor-id="xavier_param">xavier_param</h3>
<blockquote class="blockquote">
<pre><code> xavier_param (*shape)</code></pre>
</blockquote>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L57" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="zero_param" class="level3">
<h3 class="anchored" data-anchor-id="zero_param">zero_param</h3>
<blockquote class="blockquote">
<pre><code> zero_param (*shape)</code></pre>
</blockquote>
</section>
</section>
<section id="networks" class="level2">
<h2 class="anchored" data-anchor-id="networks">Networks</h2>
<section id="networks-for-sequences" class="level3">
<h3 class="anchored" data-anchor-id="networks-for-sequences">Networks for sequences</h3>
<p>The seq networks take the sequence and possible accompanying information such as charge state or modification and apply neural network transformations.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L114" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seqcnn" class="level3">
<h3 class="anchored" data-anchor-id="seqcnn">SeqCNN</h3>
<blockquote class="blockquote">
<pre><code> SeqCNN (embedding_hidden)</code></pre>
</blockquote>
<p>Extract sequence features using <code>torch.nn.Conv1D</code> with different kernel sizes (1(residue connection),3,5,7), and then concatenate the outputs of these Conv1Ds. The Output dim is 4*embedding_hidden.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L68" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seqcnn_multikernel" class="level3">
<h3 class="anchored" data-anchor-id="seqcnn_multikernel">SeqCNN_MultiKernel</h3>
<blockquote class="blockquote">
<pre><code> SeqCNN_MultiKernel (out_features:int)</code></pre>
</blockquote>
<p>Extract sequence features using <code>torch.nn.Conv1D</code> with different kernel sizes (1(residue connection),3,5,7), and then concatenate the outputs of these Conv1Ds.</p>
<section id="pytorch-built-in-transformers-for-test-only" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-built-in-transformers-for-test-only">PyTorch Built-in Transformers (for test only)</h4>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L168" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
</section>
<section id="hidden_transformer" class="level3">
<h3 class="anchored" data-anchor-id="hidden_transformer">Hidden_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Hidden_Transformer (hidden, hidden_expand=4, nheads=8, nlayers=4,
                     dropout=0.1)</code></pre>
</blockquote>
<p>Transformer NN based on pytorch’s built-in TransformerLayer class</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L145" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seq_transformer" class="level3">
<h3 class="anchored" data-anchor-id="seq_transformer">Seq_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Seq_Transformer (in_features, hidden_features, nheads=8, nlayers=2,
                  dropout=0.1)</code></pre>
</blockquote>
<p>Using PyTorch built-in Transformer layers</p>
<section id="huggingface-transformers" class="level4">
<h4 class="anchored" data-anchor-id="huggingface-transformers">HuggingFace Transformers</h4>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L245" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
</section>
<section id="hface_transformer_with_positionalencoder" class="level3">
<h3 class="anchored" data-anchor-id="hface_transformer_with_positionalencoder">HFace_Transformer_with_PositionalEncoder</h3>
<blockquote class="blockquote">
<pre><code> HFace_Transformer_with_PositionalEncoder (hidden_dim:int,
                                           hidden_expand=4, nheads=8,
                                           nlayers=4, dropout=0.1,
                                           output_attentions=False,
                                           max_len=200)</code></pre>
</blockquote>
<p>HuggingFace transformer with a positional encoder in front.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>hidden_dim</td>
<td>int</td>
<td></td>
<td>Input and output feature dimension.</td>
</tr>
<tr class="even">
<td>hidden_expand</td>
<td>int</td>
<td>4</td>
<td>FFN hidden size = hidden*hidden_expand. Defaults to 4.</td>
</tr>
<tr class="odd">
<td>nheads</td>
<td>int</td>
<td>8</td>
<td></td>
</tr>
<tr class="even">
<td>nlayers</td>
<td>int</td>
<td>4</td>
<td>Number of transformer layers. Defaults to 4.</td>
</tr>
<tr class="odd">
<td>dropout</td>
<td>float</td>
<td>0.1</td>
<td>Dropout rate. Defaults to 0.1.</td>
</tr>
<tr class="even">
<td>output_attentions</td>
<td>bool</td>
<td>False</td>
<td>If output attention values. Defaults to False.</td>
</tr>
<tr class="odd">
<td>max_len</td>
<td>int</td>
<td>200</td>
<td>Max input sequence length. Defaults to 200.</td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L210" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="hidden_hface_transformer" class="level3">
<h3 class="anchored" data-anchor-id="hidden_hface_transformer">Hidden_HFace_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Hidden_HFace_Transformer (hidden_dim, hidden_expand=4, nheads=8,
                           nlayers=4, dropout=0.1,
                           output_attentions=False)</code></pre>
</blockquote>
<p>Transformer NN based on HuggingFace’s BertEncoder class</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L303" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seqlstm" class="level3">
<h3 class="anchored" data-anchor-id="seqlstm">SeqLSTM</h3>
<blockquote class="blockquote">
<pre><code> SeqLSTM (in_features, out_features, rnn_layer=2, bidirectional=True)</code></pre>
</blockquote>
<p>returns LSTM applied on sequence input</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L342" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seqgru" class="level3">
<h3 class="anchored" data-anchor-id="seqgru">SeqGRU</h3>
<blockquote class="blockquote">
<pre><code> SeqGRU (in_features, out_features, rnn_layer=2, bidirectional=True)</code></pre>
</blockquote>
<p>returns GRU applied on sequence input</p>
<section id="linear-seq-transformations" class="level4">
<h4 class="anchored" data-anchor-id="linear-seq-transformations">Linear Seq Transformations</h4>
<p>Takes in a sequence and applies a linear transformation on it</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L378" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
</section>
<section id="seqattentionsum" class="level3">
<h3 class="anchored" data-anchor-id="seqattentionsum">SeqAttentionSum</h3>
<blockquote class="blockquote">
<pre><code> SeqAttentionSum (in_features)</code></pre>
</blockquote>
<p>apply linear transformation and tensor rescaling with softmax</p>
<section id="positional-encoding-and-embedding" class="level4">
<h4 class="anchored" data-anchor-id="positional-encoding-and-embedding">Positional Encoding and Embedding</h4>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L415" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
</section>
<section id="positionalembedding" class="level3">
<h3 class="anchored" data-anchor-id="positionalembedding">PositionalEmbedding</h3>
<blockquote class="blockquote">
<pre><code> PositionalEmbedding (out_features=128, max_len=200)</code></pre>
</blockquote>
<p>transform sequence with the standard embedding function</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L394" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="positionalencoding">PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> PositionalEncoding (out_features=128, max_len=200)</code></pre>
</blockquote>
<p>transform sequence input into a positional representation</p>
</section>
<section id="input-networks" class="level3">
<h3 class="anchored" data-anchor-id="input-networks">Input Networks</h3>
<p>The ‘Input’ classes represent the input layers of the networks, meaning they interact directly with the (formatted) features such as the peptide sequence, the charge state, the modifications or the collision energy</p>
<section id="linear-input-transformations-and-embeddings." class="level4">
<h4 class="anchored" data-anchor-id="linear-input-transformations-and-embeddings.">Linear input transformations and embeddings.</h4>
<p>Performing embedding and linear operations on the input</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L568" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
</section>
<section id="input_aa_mod_charge_positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="input_aa_mod_charge_positionalencoding">Input_AA_Mod_Charge_PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> Input_AA_Mod_Charge_PositionalEncoding (out_features, max_len=200)</code></pre>
</blockquote>
<p>Embed AA (128 ASCII codes), modification, and charge state</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L546" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="input_aa_mod_positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="input_aa_mod_positionalencoding">Input_AA_Mod_PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> Input_AA_Mod_PositionalEncoding (out_features, max_len=200)</code></pre>
</blockquote>
<p>Encodes AA (ASCII codes) and modification vector</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L522" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="input_26aa_mod_positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_positionalencoding">Input_26AA_Mod_PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_PositionalEncoding (out_features, max_len=200)</code></pre>
</blockquote>
<p>Encodes AA (26 AA letters) and modification vector</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L502" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="mod_embedding" class="level3">
<h3 class="anchored" data-anchor-id="mod_embedding">Mod_Embedding</h3>
<blockquote class="blockquote">
<pre><code> Mod_Embedding (out_features)</code></pre>
</blockquote>
<p>Encodes the modification vector in a single layer feed forward network</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L480" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="aa_mod_embedding" class="level3">
<h3 class="anchored" data-anchor-id="aa_mod_embedding">AA_Mod_Embedding</h3>
<blockquote class="blockquote">
<pre><code> AA_Mod_Embedding (out_features, mod_feature_size=8)</code></pre>
</blockquote>
<p>Concatenates the AA (128 ASCII codes) embedding with the modifcation vector</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L456" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="mod_embedding_fixfirstk" class="level3">
<h3 class="anchored" data-anchor-id="mod_embedding_fixfirstk">Mod_Embedding_FixFirstK</h3>
<blockquote class="blockquote">
<pre><code> Mod_Embedding_FixFirstK (out_features)</code></pre>
</blockquote>
<p>Encodes the modification vector in a single layer feed forward network, but not transforming the first k features</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L432" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="meta_embedding" class="level3">
<h3 class="anchored" data-anchor-id="meta_embedding">Meta_Embedding</h3>
<blockquote class="blockquote">
<pre><code> Meta_Embedding (out_features)</code></pre>
</blockquote>
<p>Encodes Charge state, Normalized Collision Energy (NCE) and Instrument for a given spectrum into a ‘meta’ single layer network</p>
</section>
</section>
<section id="lstm" class="level2">
<h2 class="anchored" data-anchor-id="lstm">LSTM</h2>
<p>Applying LSTMs to the input</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L653" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="input_26aa_mod_charge_lstm" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_charge_lstm">Input_26AA_Mod_Charge_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_Charge_LSTM (out_features)</code></pre>
</blockquote>
<p>Applies a LSTM network to a AA (26 AA letters) sequence and modifications, and concatenates with charge state information</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L620" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="input_26aa_mod_meta_lstm" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_meta_lstm">Input_26AA_Mod_Meta_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_Meta_LSTM (out_features)</code></pre>
</blockquote>
<p>Applies a LSTM network to a AA (26 AA letters) sequence and modifications, and concatenates with ‘meta’ information (charge, nce, instrument_indices)</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L595" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="input_26aa_mod_lstm" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_lstm">Input_26AA_Mod_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_LSTM (out_features, n_lstm_layers=1)</code></pre>
</blockquote>
<p>Applies an LSTM network to a AA (26 AA letters) sequence &amp; modifications</p>
</section>
<section id="complex-seq-layers-or-output-layers" class="level3">
<h3 class="anchored" data-anchor-id="complex-seq-layers-or-output-layers">Complex Seq Layers (or Output Layers)</h3>
<p>The ‘Output’ classes represent the output layers of the networks, meaning they take the hidden layer of a network as input, transform it into the output such as a spectrum, ccs value, rt</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L710" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seq_meta_linear" class="level3">
<h3 class="anchored" data-anchor-id="seq_meta_linear">Seq_Meta_Linear</h3>
<blockquote class="blockquote">
<pre><code> Seq_Meta_Linear (in_features, out_features)</code></pre>
</blockquote>
<p>takes a hidden linear which processed the ‘meta’ information of NCE, Instrument, Charge</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L684" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="seq_meta_lstm" class="level3">
<h3 class="anchored" data-anchor-id="seq_meta_lstm">Seq_Meta_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Seq_Meta_LSTM (in_features, out_features)</code></pre>
</blockquote>
<p>Takes a hidden layer which processes the hidden tensor as well as the ‘meta’ information of NCE, Instrument, Charge</p>
</section>
<section id="encoders" class="level3">
<h3 class="anchored" data-anchor-id="encoders">Encoders</h3>
<p>The encoder classes transform the features into a learned representation. Within the encoder, the <code>Input..</code> classes are used to transform the features into a network representation. Subsequently, Convolutional Neural Networks (CNNs) and/or Long Short-Term Memory (LSTM) Networks and/or Linear transformations are applied to the data.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L951" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_26aa_mod_charge_cnn_lstm_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_charge_cnn_lstm_attnsum">Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum (out_features)</code></pre>
</blockquote>
<p>Encode AAs (26 AA letters), modifications and charge by transformers, and then by ‘SeqAttentionSum’</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L928" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_aa_mod_charge_transformer_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_charge_transformer_attnsum">Encoder_AA_Mod_Charge_Transformer_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Charge_Transformer_AttnSum (out_features, dropout=0.1,
                                            nlayers=4,
                                            output_attentions=False)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes), modifications and charge by transformers, and then by ‘SeqAttentionSum’</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L897" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_aa_mod_charge_transformer" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_charge_transformer">Encoder_AA_Mod_Charge_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Charge_Transformer (out_features, dropout=0.1, nlayers=4,
                                    output_attentions=False)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes), modifications and charge by transformers.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L874" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_aa_mod_transformer_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_transformer_attnsum">Encoder_AA_Mod_Transformer_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Transformer_AttnSum (out_features, dropout=0.1, nlayers=4,
                                     output_attentions=False)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes) and modifications by transformers.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L841" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_aa_mod_transformer" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_transformer">Encoder_AA_Mod_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Transformer (out_features, dropout=0.1, nlayers=4,
                             output_attentions=False)</code></pre>
</blockquote>
<p>AAs (128 ASCII codes) and modifications embedded by CNN and LSTM layers, then encoded by ‘SeqAttentionSum’.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L813" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_aa_mod_cnn_lstm_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_cnn_lstm_attnsum">Encoder_AA_Mod_CNN_LSTM_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_CNN_LSTM_AttnSum (out_features, n_lstm_layers=2)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes) and modifications by CNN and LSTM layers, and then by ‘SeqAttentionSum’.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L784" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_26aa_mod_cnn_lstm_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_cnn_lstm_attnsum">Encoder_26AA_Mod_CNN_LSTM_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_CNN_LSTM_AttnSum (out_features, n_lstm_layers=2)</code></pre>
</blockquote>
<p>Encode AAs (26 AA letters) and modifications by CNN and LSTM layers, then by ‘SeqAttentionSum’.</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L758" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_26aa_mod_cnn_lstm" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_cnn_lstm">Encoder_26AA_Mod_CNN_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_CNN_LSTM (out_features, n_lstm_layers=1)</code></pre>
</blockquote>
<p>Encode AAs (26 AA letters) and modifications by CNN and LSTM layers</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L736" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="encoder_26aa_mod_lstm" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_lstm">Encoder_26AA_Mod_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_LSTM (out_features, n_lstm_layers=1)</code></pre>
</blockquote>
<p>Two LSTM layers on AA (26 AA letters) and modifications.</p>
</section>
<section id="decoders" class="level3">
<h3 class="anchored" data-anchor-id="decoders">Decoders</h3>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L1011" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="decoder_gru" class="level3">
<h3 class="anchored" data-anchor-id="decoder_gru">Decoder_GRU</h3>
<blockquote class="blockquote">
<pre><code> Decoder_GRU (in_features, out_features)</code></pre>
</blockquote>
<p>Decode with GRU</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L985" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="decoder_lstm" class="level3">
<h3 class="anchored" data-anchor-id="decoder_lstm">Decoder_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Decoder_LSTM (in_features, out_features)</code></pre>
</blockquote>
<p>Decode with LSTM</p>
<hr>
<p><a href="https://github.com/MannLabs/alphapeptdeep/blob/main/peptdeep/model/building_block.py#L1038" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="decoder_linear" class="level3">
<h3 class="anchored" data-anchor-id="decoder_linear">Decoder_Linear</h3>
<blockquote class="blockquote">
<pre><code> Decoder_Linear (in_features, out_features)</code></pre>
</blockquote>
<p>Decode w linear NN</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>