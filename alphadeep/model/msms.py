# AUTOGENERATED! DO NOT EDIT! File to edit: nbdev_nbs/model/msms.ipynb (unless otherwise specified).

__all__ = ['ModelMSMSpDeep3', 'IntenAwareLoss', 'pDeepModel', 'mod_feature_size', 'max_instrument_num', 'frag_types',
           'max_frag_charge', 'num_ion_types', 'nce_factor', 'charge_factor', 'cosine', 'spectral_angle', 'pearson',
           'spearman', 'get_metric_fuc', 'batch_metric', 'evaluate_msms', 'add_cutoff_metric']

# Cell
import torch
import pandas as pd
import numpy as np

from tqdm import tqdm

from alphabase.peptide.fragment import \
    init_fragment_by_precursor_dataframe, \
    set_sliced_fragment_dataframe, \
    get_sliced_fragment_dataframe, \
    get_charged_frag_types

from alphadeep.model.featurize import \
    parse_aa_indices, parse_instrument_indices, \
    get_batch_mod_feature

from alphadeep._settings import \
    global_settings as settings, \
    const_settings

import alphadeep.model.base as model_base

class ModelMSMSpDeep3(torch.nn.Module):
    def __init__(self,
        mod_feature_size,
        num_ion_types,
        max_instrument_num,
        dropout=0.2
    ):
        super().__init__()
        BiRNN = True
        aa_embedding_hidden = 27
        hidden=256
        ins_nce_embed_size = 3

        self.max_instrument_num = max_instrument_num
        self.instrument_nce_embed = torch.nn.Linear(max_instrument_num+1,ins_nce_embed_size)
        # ins_nce_embed_size = conf.max_instrument_num+1
        # self.instrument_nce_embed = torch.nn.Identity()

        output_hidden_size = hidden*(2 if BiRNN else 1) + ins_nce_embed_size + 1

        # mod_embed_size = 8
        # self.mod_embed_weights = torch.nn.Parameter(
            # torch.empty(mod_size, mod_embed_size),
            # requires_grad = True
        # )
        self.dropout = torch.nn.Dropout(dropout)

        self.input = model_base.SeqLSTM(
            aa_embedding_hidden+mod_feature_size+ins_nce_embed_size+1,
            hidden,
            rnn_layer=1, bidirectional=BiRNN
        )

        self.hidden = model_base.SeqLSTM(
            output_hidden_size,
            hidden,
            rnn_layer=1, bidirectional=BiRNN
        )

        self.output = model_base.SeqLSTM(
            output_hidden_size,
            num_ion_types,
            rnn_layer=1, bidirectional=False
        )

    def forward(self,
        aa_indices,
        mod_x,
        charges:torch.Tensor,
        NCEs:torch.Tensor,
        instrument_indices,
    ):
        aa_x = torch.nn.functional.one_hot(aa_indices, 27)
        inst_x = torch.nn.functional.one_hot(instrument_indices, self.max_instrument_num)

        ins_nce = torch.cat((inst_x, NCEs), 1)
        ins_nce = self.instrument_nce_embed(ins_nce)
        ins_nce_charge = torch.cat((ins_nce, charges), 1)
        ins_nce_charge = ins_nce_charge.unsqueeze(1).repeat(1, aa_x.size(1), 1)

        x = torch.cat((aa_x, mod_x, ins_nce_charge), 2)
        x = self.input(x)
        x = self.dropout(x)

        x = torch.cat((x, ins_nce_charge), 2)
        x = self.hidden(x)
        x = self.dropout(x)

        x = torch.cat((x, ins_nce_charge), 2)

        return self.output(x[:,1:-2,:])


# Cell
class IntenAwareLoss(torch.nn.Module):
    def __init__(self, base_weight = 0.1):
        super().__init__()
        self.w = base_weight

    def forward(self, pred, target):
        x = pred.reshape(-1)
        y = target.reshape(-1)
        #return torch.sum(torch.abs(x-y))/pred.size(0)
        return torch.mean((y+self.w)*torch.abs(x-y))

# Cell

mod_feature_size = len(const_settings['mod_elements'])
max_instrument_num = const_settings['max_instrument_num']
frag_types = settings['model']['frag_types']
max_frag_charge = settings['model']['max_frag_charge']
num_ion_types = len(frag_types)*max_frag_charge
nce_factor = const_settings['nce_factor']
charge_factor = const_settings['charge_factor']

class pDeepModel(model_base.ModelImplBase):
    def __init__(self,
        dropout=0.2,
        lr=0.001,
    ):
        super().__init__()
        self.charged_frag_types = get_charged_frag_types(
            frag_types, max_frag_charge
        )
        self.charge_factor = charge_factor
        self.NCE_factor = nce_factor
        self.build(
            ModelMSMSpDeep3,
            mod_feature_size = mod_feature_size,
            num_ion_types = len(self.charged_frag_types),
            max_instrument_num = max_instrument_num,
            dropout=dropout,
            lr=lr
        )
        self.loss_func = IntenAwareLoss()
        # self.loss_func = torch.nn.L1Loss()
        self.min_inten = 1e-4

    def train(self,
        precursor_df: pd.DataFrame,
        fragment_inten_df: pd.DataFrame,
        epoch=10,
        batch_size=1024,
        verbose=True
    ):
        self.model.train()

        fragment_inten_df = fragment_inten_df[self.charged_frag_types]

        if np.all(precursor_df['NCE'].values > 1):
            precursor_df['NCE'] = precursor_df['NCE']*self.NCE_factor

        for epoch in range(epoch):
            batch_cost = []
            _grouped = list(precursor_df.sample(frac=1).groupby('nAA'))
            rnd_nAA = np.random.permutation(len(_grouped))
            batch_tqdm = tqdm(rnd_nAA)
            for i_group in batch_tqdm:
                nAA, df_group = _grouped[i_group]
                df_group = df_group.reset_index(drop=True)
                for i in range(0, len(df_group), batch_size):
                    batch_end = i+batch_size-1 # DataFrame.loc[start:end] inlcudes the end

                    aa_indices = torch.LongTensor(
                        parse_aa_indices(
                            df_group.loc[i:batch_end, 'sequence'].values.astype('U')
                        )
                    )

                    mod_x_batch = get_batch_mod_feature(df_group.loc[i:batch_end,:], nAA)
                    mod_x = torch.Tensor(mod_x_batch)

                    charges = torch.Tensor(
                        df_group.loc[i:batch_end, 'charge'].values
                    ).unsqueeze(1)*self.charge_factor

                    nces = torch.Tensor(df_group.loc[i:batch_end, 'NCE'].values).unsqueeze(1)

                    instrument_indices = torch.LongTensor(
                        parse_instrument_indices(df_group.loc[i:batch_end, 'instrument'])
                    )
                    intens = torch.Tensor(
                        get_sliced_fragment_dataframe(
                            fragment_inten_df,
                            df_group.loc[
                                i:batch_end, ['frag_start_idx','frag_end_idx']
                            ].values
                        ).values
                    ).view(-1, nAA-1, len(self.charged_frag_types))

                    cost = self._train_one_batch(
                        intens,
                        aa_indices, mod_x, charges,
                        nces, instrument_indices
                    )
                    batch_cost.append(cost.item())
                batch_tqdm.set_description(
                    f'Epoch={epoch+1}, nAA={nAA}, Batch={len(batch_cost)}, Loss={cost.item():.4f}'
                )
            if verbose: print(f'[MS/MS training] epoch={epoch+1}, mean Loss={np.mean(batch_cost)}')

    def predict(self,
        precursor_df: pd.DataFrame,
        batch_size: int=1024
    )->pd.DataFrame:

        self.model.eval()

        predict_inten_df = init_fragment_by_precursor_dataframe(
            precursor_df, self.charged_frag_types
        )

        if np.all(precursor_df['NCE'].values > 1):
            precursor_df['NCE'] = precursor_df['NCE']*self.NCE_factor

        _grouped = precursor_df.groupby('nAA')
        for nAA, df_group in tqdm(_grouped):
            df_group = df_group.reset_index(drop=True)
            for i in range(0, len(df_group), batch_size):
                batch_end = i+batch_size-1 # DataFrame.loc[start:end] inlcudes the end

                mod_x_batch = get_batch_mod_feature(df_group.loc[i:batch_end,:], nAA)

                aa_indices = torch.LongTensor(parse_aa_indices(
                    df_group.loc[i:batch_end, 'sequence'].values.astype('U')
                ))
                mod_x = torch.Tensor(mod_x_batch)
                charges = torch.Tensor(
                    df_group.loc[i:batch_end, 'charge'].values
                ).view(-1,1)*self.charge_factor

                nces = torch.Tensor(df_group.loc[i:batch_end, 'NCE'].values).view(-1,1)
                instrument_indices = torch.LongTensor(
                    parse_instrument_indices(df_group.loc[i:batch_end, 'instrument'])
                )

                predicts = self.model(
                    *[fea.to(self.device) for fea in
                    [aa_indices, mod_x, charges, nces, instrument_indices]
                ]).cpu().detach().numpy()
                predicts[predicts>1] = 1
                predicts[predicts<self.min_inten] = 0

                set_sliced_fragment_dataframe(
                    predict_inten_df,
                    predicts.reshape((-1, len(self.charged_frag_types))),
                    df_group.loc[
                        i:batch_end,
                        ['frag_start_idx','frag_end_idx']
                    ].values,
                    self.charged_frag_types
                )

        return predict_inten_df

# Cell
from scipy.stats import pearsonr
from scipy.stats import spearmanr
from scipy.spatial.distance import cosine
import typing

def cosine(x1, x2, eps=1e-8):
    _cos = np.dot(x1,x2)/(np.linalg.norm(x1)*np.linalg.norm(x2)+eps)
    if _cos > 1: _cos = 1
    return _cos

def spectral_angle(x1, x2, eps=1e-8):
    return 1 - 2 * np.arccos(cosine(x1,x2,eps)) / np.pi

def pearson(x1, x2):
    ret = pearsonr(x1, x2)[0]
    if np.isnan(ret):
        return 0
    else: return ret

def spearman(x1, x2):
    ret = spearmanr(x1, x2)[0]
    if np.isnan(ret):
        return 0
    else: return ret

def get_metric_fuc(metric):
    metric = metric.upper()
    if metric == 'COS':
        return cosine
    elif metric == 'PCC':
        return pearson
    elif metric == 'SPC':
        return spearman
    elif metric == 'SA':
        return spectral_angle
    else:
        raise NotImplementedError(f'Unknown metric "{metric}".')

def batch_metric(batch1, batch2, frag_start_end_list, metric_func):
    sim_list = []
    for start, end in frag_start_end_list:
        sim_list.append(
            metric_func(
                batch1[start:end].reshape(-1),
                batch2[start:end].reshape(-1)
            )
        )
    return sim_list

def evaluate_msms(
    precursor_df: pd.DataFrame,
    predict_inten_df: pd.DataFrame,
    fragment_inten_df: pd.DataFrame,
    charged_frag_types: typing.List,
    metrics = ['PCC','COS','SPC','SA'],
)->pd.DataFrame:
    ret = pd.DataFrame(
        np.zeros((len(precursor_df), len(metrics))),
        columns=metrics
    )

    for metric in tqdm(metrics):
        ret[metric] = batch_metric(
            predict_inten_df[charged_frag_types].values,
            fragment_inten_df[charged_frag_types].values,
            precursor_df[['frag_start_idx','frag_end_idx']].values,
            get_metric_fuc(metric)
        )
    return ret

def add_cutoff_metric(
    metrics_describ, metrics_df, thres=0.9
):
    vals = []
    for col in metrics_describ.columns.values:
        vals.append(metrics_df.loc[metrics_df[col]>thres, col].count()/len(metrics_df))
    metrics_describ.loc[f'>{thres:.2f}'] = vals
    return metrics_describ