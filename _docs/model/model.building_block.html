<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.149">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>alphapeptdeep - NN Building Block</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="alphapeptdeep - NN Building Block">
<meta property="og:description" content="The building block module specifies the architectures of the core neural networks used in PeptDeep.">
<meta property="og:site-name" content="alphapeptdeep">
<meta name="twitter:title" content="alphapeptdeep - NN Building Block">
<meta name="twitter:description" content="The building block module specifies the architectures of the core neural networks used in PeptDeep.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">alphapeptdeep</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/MannLabs/alphapeptdeep/tree/main/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">NN Building Block</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Index</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pipeline_api.html" class="sidebar-item-text sidebar-link">The high-level pipeline APIs</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pretrained_models.html" class="sidebar-item-text sidebar-link">Integrated functionalities for MS2/RT/CCS models</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../settings.html" class="sidebar-item-text sidebar-link">All global settings for peptdeep</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../test_ui.html" class="sidebar-item-text sidebar-link">Testing GUI</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../utils.html" class="sidebar-item-text sidebar-link">Utils</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">mass_spec</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mass_spec/mass_spec.mass_calibration.html" class="sidebar-item-text sidebar-link">Mass calibration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mass_spec/mass_spec.match.html" class="sidebar-item-text sidebar-link">Peak matching functionalities</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mass_spec/mass_spec.ms_reader.html" class="sidebar-item-text sidebar-link">Basic spectrum file readers</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">model</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.building_block.html" class="sidebar-item-text sidebar-link active">NN Building Block</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.ccs.html" class="sidebar-item-text sidebar-link">CCS model and interface</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.featurize.html" class="sidebar-item-text sidebar-link">Functionalities for featurization/tensorization</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.generic_property_prediction.html" class="sidebar-item-text sidebar-link">Generic Property Prediction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.model_interface.html" class="sidebar-item-text sidebar-link">Model Interface</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.model_shop.html" class="sidebar-item-text sidebar-link">legacy, see <code>generic_property_prediction.ipynb</code></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.ms2.html" class="sidebar-item-text sidebar-link">pDeepModel for MS/MS prediction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/model.rt.html" class="sidebar-item-text sidebar-link">Retention Time Prediction Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model/test_predict_mp.html" class="sidebar-item-text sidebar-link">Testing predict_mp()</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">protein</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../protein/protein.fasta.html" class="sidebar-item-text sidebar-link">Predicting libraries from fasta or sequences</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">psm_frag_reader</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/psm_frag_reader.library_frag_reader.html" class="sidebar-item-text sidebar-link">PSM reader with fragments: TSV lib</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/psm_frag_reader.maxquant_frag_reader.html" class="sidebar-item-text sidebar-link">PSM Reader with fragments: MaxQuant msms.txt</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/psm_frag_reader.psm_frag_reader.html" class="sidebar-item-text sidebar-link">PSM reader with fragments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../psm_frag_reader/psm_frag_reader.psmlabel_reader.html" class="sidebar-item-text sidebar-link">Legacy: psmlabel (pDeep) reader with fragments</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">rescore</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rescore/rescore.fdr.html" class="sidebar-item-text sidebar-link">Functionalities to calculate FDRs</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rescore/rescore.feature_extractor.html" class="sidebar-item-text sidebar-link">Functionalities to extract features from peptdeep prediction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rescore/rescore.percolator.html" class="sidebar-item-text sidebar-link">Percolator functionalities</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">spec_lib</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/spec_lib.library_factory.html" class="sidebar-item-text sidebar-link">Factory classes to predict libraries from different sources</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/spec_lib.predict_lib.html" class="sidebar-item-text sidebar-link">Base functionalities to predict libraries</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/test_translate_tsv.html" class="sidebar-item-text sidebar-link">Testing translate_tsv()</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../spec_lib/spec_lib.translate.html" class="sidebar-item-text sidebar-link">Translate peptdeep spectral libraries into other formats (e.g.&nbsp;TSV)</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#description" id="toc-description" class="nav-link active" data-scroll-target="#description">Description</a></li>
  <li><a href="#variables" id="toc-variables" class="nav-link" data-scroll-target="#variables">Variables</a></li>
  <li><a href="#utility-functions" id="toc-utility-functions" class="nav-link" data-scroll-target="#utility-functions">Utility Functions</a>
  <ul>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a></li>
  <li><a href="#instrument_embedding" id="toc-instrument_embedding" class="nav-link" data-scroll-target="#instrument_embedding">instrument_embedding</a></li>
  <li><a href="#aa_one_hot" id="toc-aa_one_hot" class="nav-link" data-scroll-target="#aa_one_hot">aa_one_hot</a></li>
  <li><a href="#ascii_embedding" id="toc-ascii_embedding" class="nav-link" data-scroll-target="#ascii_embedding">ascii_embedding</a></li>
  <li><a href="#aa_embedding" id="toc-aa_embedding" class="nav-link" data-scroll-target="#aa_embedding">aa_embedding</a></li>
  <li><a href="#initial-states" id="toc-initial-states" class="nav-link" data-scroll-target="#initial-states">Initial states</a></li>
  <li><a href="#xavier_param" id="toc-xavier_param" class="nav-link" data-scroll-target="#xavier_param">xavier_param</a></li>
  <li><a href="#zero_param" id="toc-zero_param" class="nav-link" data-scroll-target="#zero_param">zero_param</a></li>
  </ul></li>
  <li><a href="#networks" id="toc-networks" class="nav-link" data-scroll-target="#networks">Networks</a>
  <ul>
  <li><a href="#networks-for-sequences" id="toc-networks-for-sequences" class="nav-link" data-scroll-target="#networks-for-sequences">Networks for sequences</a></li>
  <li><a href="#seqcnn" id="toc-seqcnn" class="nav-link" data-scroll-target="#seqcnn">SeqCNN</a></li>
  <li><a href="#seqcnn_multikernel" id="toc-seqcnn_multikernel" class="nav-link" data-scroll-target="#seqcnn_multikernel">SeqCNN_MultiKernel</a>
  <ul class="collapse">
  <li><a href="#pytorch-built-in-transformers-for-test-only" id="toc-pytorch-built-in-transformers-for-test-only" class="nav-link" data-scroll-target="#pytorch-built-in-transformers-for-test-only">PyTorch Built-in Transformers (for test only)</a></li>
  </ul></li>
  <li><a href="#hidden_transformer" id="toc-hidden_transformer" class="nav-link" data-scroll-target="#hidden_transformer">Hidden_Transformer</a></li>
  <li><a href="#seq_transformer" id="toc-seq_transformer" class="nav-link" data-scroll-target="#seq_transformer">Seq_Transformer</a>
  <ul class="collapse">
  <li><a href="#huggingface-transformers" id="toc-huggingface-transformers" class="nav-link" data-scroll-target="#huggingface-transformers">HuggingFace Transformers</a></li>
  </ul></li>
  <li><a href="#hface_transformer_with_positionalencoder" id="toc-hface_transformer_with_positionalencoder" class="nav-link" data-scroll-target="#hface_transformer_with_positionalencoder">HFace_Transformer_with_PositionalEncoder</a></li>
  <li><a href="#hidden_hface_transformer" id="toc-hidden_hface_transformer" class="nav-link" data-scroll-target="#hidden_hface_transformer">Hidden_HFace_Transformer</a></li>
  <li><a href="#seqlstm" id="toc-seqlstm" class="nav-link" data-scroll-target="#seqlstm">SeqLSTM</a></li>
  <li><a href="#seqgru" id="toc-seqgru" class="nav-link" data-scroll-target="#seqgru">SeqGRU</a>
  <ul class="collapse">
  <li><a href="#linear-seq-transformations" id="toc-linear-seq-transformations" class="nav-link" data-scroll-target="#linear-seq-transformations">Linear Seq Transformations</a></li>
  </ul></li>
  <li><a href="#seqattentionsum" id="toc-seqattentionsum" class="nav-link" data-scroll-target="#seqattentionsum">SeqAttentionSum</a>
  <ul class="collapse">
  <li><a href="#positional-encoding-and-embedding" id="toc-positional-encoding-and-embedding" class="nav-link" data-scroll-target="#positional-encoding-and-embedding">Positional Encoding and Embedding</a></li>
  </ul></li>
  <li><a href="#positionalembedding" id="toc-positionalembedding" class="nav-link" data-scroll-target="#positionalembedding">PositionalEmbedding</a></li>
  <li><a href="#positionalencoding" id="toc-positionalencoding" class="nav-link" data-scroll-target="#positionalencoding">PositionalEncoding</a></li>
  <li><a href="#input-networks" id="toc-input-networks" class="nav-link" data-scroll-target="#input-networks">Input Networks</a>
  <ul class="collapse">
  <li><a href="#linear-input-transformations-and-embeddings." id="toc-linear-input-transformations-and-embeddings." class="nav-link" data-scroll-target="#linear-input-transformations-and-embeddings.">Linear input transformations and embeddings.</a></li>
  </ul></li>
  <li><a href="#input_aa_mod_charge_positionalencoding" id="toc-input_aa_mod_charge_positionalencoding" class="nav-link" data-scroll-target="#input_aa_mod_charge_positionalencoding">Input_AA_Mod_Charge_PositionalEncoding</a></li>
  <li><a href="#input_aa_mod_positionalencoding" id="toc-input_aa_mod_positionalencoding" class="nav-link" data-scroll-target="#input_aa_mod_positionalencoding">Input_AA_Mod_PositionalEncoding</a></li>
  <li><a href="#input_26aa_mod_positionalencoding" id="toc-input_26aa_mod_positionalencoding" class="nav-link" data-scroll-target="#input_26aa_mod_positionalencoding">Input_26AA_Mod_PositionalEncoding</a></li>
  <li><a href="#mod_embedding" id="toc-mod_embedding" class="nav-link" data-scroll-target="#mod_embedding">Mod_Embedding</a></li>
  <li><a href="#aa_mod_embedding" id="toc-aa_mod_embedding" class="nav-link" data-scroll-target="#aa_mod_embedding">AA_Mod_Embedding</a></li>
  <li><a href="#mod_embedding_fixfirstk" id="toc-mod_embedding_fixfirstk" class="nav-link" data-scroll-target="#mod_embedding_fixfirstk">Mod_Embedding_FixFirstK</a></li>
  <li><a href="#meta_embedding" id="toc-meta_embedding" class="nav-link" data-scroll-target="#meta_embedding">Meta_Embedding</a>
  <ul class="collapse">
  <li><a href="#lstm" id="toc-lstm" class="nav-link" data-scroll-target="#lstm">LSTM</a></li>
  </ul></li>
  <li><a href="#input_26aa_mod_charge_lstm" id="toc-input_26aa_mod_charge_lstm" class="nav-link" data-scroll-target="#input_26aa_mod_charge_lstm">Input_26AA_Mod_Charge_LSTM</a></li>
  <li><a href="#input_26aa_mod_meta_lstm" id="toc-input_26aa_mod_meta_lstm" class="nav-link" data-scroll-target="#input_26aa_mod_meta_lstm">Input_26AA_Mod_Meta_LSTM</a></li>
  <li><a href="#input_26aa_mod_lstm" id="toc-input_26aa_mod_lstm" class="nav-link" data-scroll-target="#input_26aa_mod_lstm">Input_26AA_Mod_LSTM</a></li>
  <li><a href="#complex-seq-layers-or-output-layers" id="toc-complex-seq-layers-or-output-layers" class="nav-link" data-scroll-target="#complex-seq-layers-or-output-layers">Complex Seq Layers (or Output Layers)</a></li>
  <li><a href="#seq_meta_linear" id="toc-seq_meta_linear" class="nav-link" data-scroll-target="#seq_meta_linear">Seq_Meta_Linear</a></li>
  <li><a href="#seq_meta_lstm" id="toc-seq_meta_lstm" class="nav-link" data-scroll-target="#seq_meta_lstm">Seq_Meta_LSTM</a></li>
  <li><a href="#encoders" id="toc-encoders" class="nav-link" data-scroll-target="#encoders">Encoders</a></li>
  <li><a href="#encoder_26aa_mod_charge_cnn_lstm_attnsum" id="toc-encoder_26aa_mod_charge_cnn_lstm_attnsum" class="nav-link" data-scroll-target="#encoder_26aa_mod_charge_cnn_lstm_attnsum">Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum</a></li>
  <li><a href="#encoder_aa_mod_charge_transformer_attnsum" id="toc-encoder_aa_mod_charge_transformer_attnsum" class="nav-link" data-scroll-target="#encoder_aa_mod_charge_transformer_attnsum">Encoder_AA_Mod_Charge_Transformer_AttnSum</a></li>
  <li><a href="#encoder_aa_mod_charge_transformer" id="toc-encoder_aa_mod_charge_transformer" class="nav-link" data-scroll-target="#encoder_aa_mod_charge_transformer">Encoder_AA_Mod_Charge_Transformer</a></li>
  <li><a href="#encoder_aa_mod_transformer_attnsum" id="toc-encoder_aa_mod_transformer_attnsum" class="nav-link" data-scroll-target="#encoder_aa_mod_transformer_attnsum">Encoder_AA_Mod_Transformer_AttnSum</a></li>
  <li><a href="#encoder_aa_mod_transformer" id="toc-encoder_aa_mod_transformer" class="nav-link" data-scroll-target="#encoder_aa_mod_transformer">Encoder_AA_Mod_Transformer</a></li>
  <li><a href="#encoder_aa_mod_cnn_lstm_attnsum" id="toc-encoder_aa_mod_cnn_lstm_attnsum" class="nav-link" data-scroll-target="#encoder_aa_mod_cnn_lstm_attnsum">Encoder_AA_Mod_CNN_LSTM_AttnSum</a></li>
  <li><a href="#encoder_26aa_mod_cnn_lstm_attnsum" id="toc-encoder_26aa_mod_cnn_lstm_attnsum" class="nav-link" data-scroll-target="#encoder_26aa_mod_cnn_lstm_attnsum">Encoder_26AA_Mod_CNN_LSTM_AttnSum</a></li>
  <li><a href="#encoder_26aa_mod_cnn_lstm" id="toc-encoder_26aa_mod_cnn_lstm" class="nav-link" data-scroll-target="#encoder_26aa_mod_cnn_lstm">Encoder_26AA_Mod_CNN_LSTM</a></li>
  <li><a href="#encoder_26aa_mod_lstm" id="toc-encoder_26aa_mod_lstm" class="nav-link" data-scroll-target="#encoder_26aa_mod_lstm">Encoder_26AA_Mod_LSTM</a></li>
  <li><a href="#decoders" id="toc-decoders" class="nav-link" data-scroll-target="#decoders">Decoders</a></li>
  <li><a href="#decoder_gru" id="toc-decoder_gru" class="nav-link" data-scroll-target="#decoder_gru">Decoder_GRU</a></li>
  <li><a href="#decoder_lstm" id="toc-decoder_lstm" class="nav-link" data-scroll-target="#decoder_lstm">Decoder_LSTM</a></li>
  <li><a href="#decoder_linear" id="toc-decoder_linear" class="nav-link" data-scroll-target="#decoder_linear">Decoder_Linear</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/MannLabs/alphapeptdeep/tree/main/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">NN Building Block</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="description" class="level2">
<h2 class="anchored" data-anchor-id="description">Description</h2>
<p>The building block module specifies the architectures of the core neural networks used in PeptDeep.</p>
<p>All networks are based on the <a href="'https://pytorch.org/'">PyTorch</a> package by subclassing <code>torch.nn.Module</code>, which is the base class for all neural networks. To implement the Transformer-network, the HuggingFace <a href="'https://huggingface.co/docs/transformers/'">transformers</a> package is used, which allows to specify transformer networks in Pytorch.</p>
</section>
<section id="variables" class="level2">
<h2 class="anchored" data-anchor-id="variables">Variables</h2>
</section>
<section id="utility-functions" class="level2">
<h2 class="anchored" data-anchor-id="utility-functions">Utility Functions</h2>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>Basic embeddings or encodings of inputs such as AA sequence or modification state</p>
<hr>
</section>
<section id="instrument_embedding" class="level3">
<h3 class="anchored" data-anchor-id="instrument_embedding">instrument_embedding</h3>
<blockquote class="blockquote">
<pre><code> instrument_embedding (hidden_size)</code></pre>
</blockquote>
<hr>
</section>
<section id="aa_one_hot" class="level3">
<h3 class="anchored" data-anchor-id="aa_one_hot">aa_one_hot</h3>
<blockquote class="blockquote">
<pre><code> aa_one_hot (aa_indices, *cat_others)</code></pre>
</blockquote>
<hr>
</section>
<section id="ascii_embedding" class="level3">
<h3 class="anchored" data-anchor-id="ascii_embedding">ascii_embedding</h3>
<blockquote class="blockquote">
<pre><code> ascii_embedding (hidden_size)</code></pre>
</blockquote>
<hr>
</section>
<section id="aa_embedding" class="level3">
<h3 class="anchored" data-anchor-id="aa_embedding">aa_embedding</h3>
<blockquote class="blockquote">
<pre><code> aa_embedding (hidden_size)</code></pre>
</blockquote>
</section>
<section id="initial-states" class="level3">
<h3 class="anchored" data-anchor-id="initial-states">Initial states</h3>
<p>Generates tensors defining the initial (hidden) states of the elements in the input sequence</p>
<hr>
</section>
<section id="xavier_param" class="level3">
<h3 class="anchored" data-anchor-id="xavier_param">xavier_param</h3>
<blockquote class="blockquote">
<pre><code> xavier_param (*shape)</code></pre>
</blockquote>
<hr>
</section>
<section id="zero_param" class="level3">
<h3 class="anchored" data-anchor-id="zero_param">zero_param</h3>
<blockquote class="blockquote">
<pre><code> zero_param (*shape)</code></pre>
</blockquote>
</section>
</section>
<section id="networks" class="level2">
<h2 class="anchored" data-anchor-id="networks">Networks</h2>
<section id="networks-for-sequences" class="level3">
<h3 class="anchored" data-anchor-id="networks-for-sequences">Networks for sequences</h3>
<p>The seq networks take the sequence and possible accompanying information such as charge state or modification and apply neural network transformations.</p>
<hr>
</section>
<section id="seqcnn" class="level3">
<h3 class="anchored" data-anchor-id="seqcnn">SeqCNN</h3>
<blockquote class="blockquote">
<pre><code> SeqCNN (embedding_hidden)</code></pre>
</blockquote>
<p>Extract sequence features using <code>torch.nn.Conv1D</code> with different kernel sizes (1(residue connection),3,5,7), and then concatenate the outputs of these Conv1Ds. The Output dim is 4*embedding_hidden.</p>
<hr>
</section>
<section id="seqcnn_multikernel" class="level3">
<h3 class="anchored" data-anchor-id="seqcnn_multikernel">SeqCNN_MultiKernel</h3>
<blockquote class="blockquote">
<pre><code> SeqCNN_MultiKernel (out_features:int)</code></pre>
</blockquote>
<p>Extract sequence features using <code>torch.nn.Conv1D</code> with different kernel sizes (1(residue connection),3,5,7), and then concatenate the outputs of these Conv1Ds.</p>
<section id="pytorch-built-in-transformers-for-test-only" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-built-in-transformers-for-test-only">PyTorch Built-in Transformers (for test only)</h4>
<hr>
</section>
</section>
<section id="hidden_transformer" class="level3">
<h3 class="anchored" data-anchor-id="hidden_transformer">Hidden_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Hidden_Transformer (hidden, hidden_expand=4, nheads=8, nlayers=4,
                     dropout=0.1)</code></pre>
</blockquote>
<p>Transformer NN based on pytorch’s built-in TransformerLayer class</p>
<hr>
</section>
<section id="seq_transformer" class="level3">
<h3 class="anchored" data-anchor-id="seq_transformer">Seq_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Seq_Transformer (in_features, hidden_features, nheads=8, nlayers=2,
                  dropout=0.1)</code></pre>
</blockquote>
<p>Using PyTorch built-in Transformer layers</p>
<section id="huggingface-transformers" class="level4">
<h4 class="anchored" data-anchor-id="huggingface-transformers">HuggingFace Transformers</h4>
<hr>
</section>
</section>
<section id="hface_transformer_with_positionalencoder" class="level3">
<h3 class="anchored" data-anchor-id="hface_transformer_with_positionalencoder">HFace_Transformer_with_PositionalEncoder</h3>
<blockquote class="blockquote">
<pre><code> HFace_Transformer_with_PositionalEncoder (hidden_dim:int,
                                           hidden_expand=4, nheads=8,
                                           nlayers=4, dropout=0.1,
                                           output_attentions=False,
                                           max_len=200)</code></pre>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool</p>
<hr>
</section>
<section id="hidden_hface_transformer" class="level3">
<h3 class="anchored" data-anchor-id="hidden_hface_transformer">Hidden_HFace_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Hidden_HFace_Transformer (hidden_dim, hidden_expand=4, nheads=8,
                           nlayers=4, dropout=0.1,
                           output_attentions=False)</code></pre>
</blockquote>
<p>Transformer NN based on HuggingFace’s BertEncoder class</p>
<hr>
</section>
<section id="seqlstm" class="level3">
<h3 class="anchored" data-anchor-id="seqlstm">SeqLSTM</h3>
<blockquote class="blockquote">
<pre><code> SeqLSTM (in_features, out_features, rnn_layer=2, bidirectional=True)</code></pre>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool</p>
<hr>
</section>
<section id="seqgru" class="level3">
<h3 class="anchored" data-anchor-id="seqgru">SeqGRU</h3>
<blockquote class="blockquote">
<pre><code> SeqGRU (in_features, out_features, rnn_layer=2, bidirectional=True)</code></pre>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool</p>
<section id="linear-seq-transformations" class="level4">
<h4 class="anchored" data-anchor-id="linear-seq-transformations">Linear Seq Transformations</h4>
<p>Takes in a sequence and applies a linear transformation on it</p>
<hr>
</section>
</section>
<section id="seqattentionsum" class="level3">
<h3 class="anchored" data-anchor-id="seqattentionsum">SeqAttentionSum</h3>
<blockquote class="blockquote">
<pre><code> SeqAttentionSum (in_features)</code></pre>
</blockquote>
<p>apply linear transformation and tensor rescaling with softmax</p>
<section id="positional-encoding-and-embedding" class="level4">
<h4 class="anchored" data-anchor-id="positional-encoding-and-embedding">Positional Encoding and Embedding</h4>
<hr>
</section>
</section>
<section id="positionalembedding" class="level3">
<h3 class="anchored" data-anchor-id="positionalembedding">PositionalEmbedding</h3>
<blockquote class="blockquote">
<pre><code> PositionalEmbedding (out_features=128, max_len=200)</code></pre>
</blockquote>
<p>transform sequence with the standard embedding function</p>
<hr>
</section>
<section id="positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="positionalencoding">PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> PositionalEncoding (out_features=128, max_len=200)</code></pre>
</blockquote>
<p>transform sequence input into a positional representation</p>
</section>
<section id="input-networks" class="level3">
<h3 class="anchored" data-anchor-id="input-networks">Input Networks</h3>
<p>The ‘Input’ classes represent the input layers of the networks, meaning they interact directly with the (formatted) features such as the peptide sequence, the charge state, the modifications or the collision energy</p>
<section id="linear-input-transformations-and-embeddings." class="level4">
<h4 class="anchored" data-anchor-id="linear-input-transformations-and-embeddings.">Linear input transformations and embeddings.</h4>
<p>Performing embedding and linear operations on the input</p>
<hr>
</section>
</section>
<section id="input_aa_mod_charge_positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="input_aa_mod_charge_positionalencoding">Input_AA_Mod_Charge_PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> Input_AA_Mod_Charge_PositionalEncoding (out_features, max_len=200)</code></pre>
</blockquote>
<p>Embed AA (128 ASCII codes), modification, and charge state</p>
<hr>
</section>
<section id="input_aa_mod_positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="input_aa_mod_positionalencoding">Input_AA_Mod_PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> Input_AA_Mod_PositionalEncoding (out_features, max_len=200)</code></pre>
</blockquote>
<p>Encodes AA (ASCII codes) and modification vector</p>
<hr>
</section>
<section id="input_26aa_mod_positionalencoding" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_positionalencoding">Input_26AA_Mod_PositionalEncoding</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_PositionalEncoding (out_features, max_len=200)</code></pre>
</blockquote>
<p>Encodes AA (26 AA letters) and modification vector</p>
<hr>
</section>
<section id="mod_embedding" class="level3">
<h3 class="anchored" data-anchor-id="mod_embedding">Mod_Embedding</h3>
<blockquote class="blockquote">
<pre><code> Mod_Embedding (out_features)</code></pre>
</blockquote>
<p>Encodes the modification vector in a single layer feed forward network</p>
<hr>
</section>
<section id="aa_mod_embedding" class="level3">
<h3 class="anchored" data-anchor-id="aa_mod_embedding">AA_Mod_Embedding</h3>
<blockquote class="blockquote">
<pre><code> AA_Mod_Embedding (out_features, mod_feature_size=8)</code></pre>
</blockquote>
<p>Concatenates the AA (128 ASCII codes) embedding with the modifcation vector</p>
<hr>
</section>
<section id="mod_embedding_fixfirstk" class="level3">
<h3 class="anchored" data-anchor-id="mod_embedding_fixfirstk">Mod_Embedding_FixFirstK</h3>
<blockquote class="blockquote">
<pre><code> Mod_Embedding_FixFirstK (out_features)</code></pre>
</blockquote>
<p>Encodes the modification vector in a single layer feed forward network, but not transforming the first k features</p>
<hr>
</section>
<section id="meta_embedding" class="level3">
<h3 class="anchored" data-anchor-id="meta_embedding">Meta_Embedding</h3>
<blockquote class="blockquote">
<pre><code> Meta_Embedding (out_features)</code></pre>
</blockquote>
<p>Encodes Charge state, Normalized Collision Energy (NCE) and Instrument for a given spectrum into a ‘meta’ single layer network</p>
<section id="lstm" class="level4">
<h4 class="anchored" data-anchor-id="lstm">LSTM</h4>
<p>Applying LSTMs to the input</p>
<hr>
</section>
</section>
<section id="input_26aa_mod_charge_lstm" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_charge_lstm">Input_26AA_Mod_Charge_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_Charge_LSTM (out_features)</code></pre>
</blockquote>
<p>Applies a LSTM network to a AA (26 AA letters) sequence and modifications, and concatenates with charge state information</p>
<hr>
</section>
<section id="input_26aa_mod_meta_lstm" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_meta_lstm">Input_26AA_Mod_Meta_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_Meta_LSTM (out_features)</code></pre>
</blockquote>
<p>Applies a LSTM network to a AA (26 AA letters) sequence and modifications, and concatenates with ‘meta’ information (charge, nce, instrument_indices)</p>
<hr>
</section>
<section id="input_26aa_mod_lstm" class="level3">
<h3 class="anchored" data-anchor-id="input_26aa_mod_lstm">Input_26AA_Mod_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Input_26AA_Mod_LSTM (out_features, n_lstm_layers=1)</code></pre>
</blockquote>
<p>Applies an LSTM network to a AA (26 AA letters) sequence &amp; modifications</p>
</section>
<section id="complex-seq-layers-or-output-layers" class="level3">
<h3 class="anchored" data-anchor-id="complex-seq-layers-or-output-layers">Complex Seq Layers (or Output Layers)</h3>
<p>The ‘Output’ classes represent the output layers of the networks, meaning they take the hidden layer of a network as input, transform it into the output such as a spectrum, ccs value, rt</p>
<hr>
</section>
<section id="seq_meta_linear" class="level3">
<h3 class="anchored" data-anchor-id="seq_meta_linear">Seq_Meta_Linear</h3>
<blockquote class="blockquote">
<pre><code> Seq_Meta_Linear (in_features, out_features)</code></pre>
</blockquote>
<p>takes a hidden linear which processed the ‘meta’ information of NCE, Instrument, Charge</p>
<hr>
</section>
<section id="seq_meta_lstm" class="level3">
<h3 class="anchored" data-anchor-id="seq_meta_lstm">Seq_Meta_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Seq_Meta_LSTM (in_features, out_features)</code></pre>
</blockquote>
<p>takes a hidden layer which processes the hidden tensor as well as the ‘meta’ information of NCE, Instrument, Charge</p>
</section>
<section id="encoders" class="level3">
<h3 class="anchored" data-anchor-id="encoders">Encoders</h3>
<p>The encoder classes transform the features into a learned representation. Within the encoder, the <code>Input..</code> classes are used to transform the features into a network representation. Subsequently, Convolutional Neural Networks (CNNs) and/or Long Short-Term Memory (LSTM) Networks and/or Linear transformations are applied to the data.</p>
<hr>
</section>
<section id="encoder_26aa_mod_charge_cnn_lstm_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_charge_cnn_lstm_attnsum">Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_Charge_CNN_LSTM_AttnSum (out_features)</code></pre>
</blockquote>
<p>Encode AAs (26 AA letters), modifications and charge by transformers, and then by ‘SeqAttentionSum’</p>
<hr>
</section>
<section id="encoder_aa_mod_charge_transformer_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_charge_transformer_attnsum">Encoder_AA_Mod_Charge_Transformer_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Charge_Transformer_AttnSum (out_features, dropout=0.1,
                                            nlayers=4,
                                            output_attentions=False)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes), modifications and charge by transformers, and then by ‘SeqAttentionSum’</p>
<hr>
</section>
<section id="encoder_aa_mod_charge_transformer" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_charge_transformer">Encoder_AA_Mod_Charge_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Charge_Transformer (out_features, dropout=0.1, nlayers=4,
                                    output_attentions=False)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes), modifications and charge by transformers.</p>
<hr>
</section>
<section id="encoder_aa_mod_transformer_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_transformer_attnsum">Encoder_AA_Mod_Transformer_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Transformer_AttnSum (out_features, dropout=0.1, nlayers=4,
                                     output_attentions=False)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes) and modifications by transformers.</p>
<hr>
</section>
<section id="encoder_aa_mod_transformer" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_transformer">Encoder_AA_Mod_Transformer</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_Transformer (out_features, dropout=0.1, nlayers=4,
                             output_attentions=False)</code></pre>
</blockquote>
<p>AAs (128 ASCII codes) and modifications embedded by CNN and LSTM layers, then encoded by ‘SeqAttentionSum’.</p>
<hr>
</section>
<section id="encoder_aa_mod_cnn_lstm_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_aa_mod_cnn_lstm_attnsum">Encoder_AA_Mod_CNN_LSTM_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_AA_Mod_CNN_LSTM_AttnSum (out_features, n_lstm_layers=2)</code></pre>
</blockquote>
<p>Encode AAs (128 ASCII codes) and modifications by CNN and LSTM layers, and then by ‘SeqAttentionSum’.</p>
<hr>
</section>
<section id="encoder_26aa_mod_cnn_lstm_attnsum" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_cnn_lstm_attnsum">Encoder_26AA_Mod_CNN_LSTM_AttnSum</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_CNN_LSTM_AttnSum (out_features, n_lstm_layers=2)</code></pre>
</blockquote>
<p>Encode AAs (26 AA letters) and modifications by CNN and LSTM layers, then by ‘SeqAttentionSum’.</p>
<hr>
</section>
<section id="encoder_26aa_mod_cnn_lstm" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_cnn_lstm">Encoder_26AA_Mod_CNN_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_CNN_LSTM (out_features, n_lstm_layers=1)</code></pre>
</blockquote>
<p>Encode AAs (26 AA letters) and modifications by CNN and LSTM layers</p>
<hr>
</section>
<section id="encoder_26aa_mod_lstm" class="level3">
<h3 class="anchored" data-anchor-id="encoder_26aa_mod_lstm">Encoder_26AA_Mod_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Encoder_26AA_Mod_LSTM (out_features, n_lstm_layers=1)</code></pre>
</blockquote>
<p>Two LSTM layers on AA (26 AA letters) and modifications.</p>
</section>
<section id="decoders" class="level3">
<h3 class="anchored" data-anchor-id="decoders">Decoders</h3>
<hr>
</section>
<section id="decoder_gru" class="level3">
<h3 class="anchored" data-anchor-id="decoder_gru">Decoder_GRU</h3>
<blockquote class="blockquote">
<pre><code> Decoder_GRU (in_features, out_features)</code></pre>
</blockquote>
<p>Decode with GRU</p>
<hr>
</section>
<section id="decoder_lstm" class="level3">
<h3 class="anchored" data-anchor-id="decoder_lstm">Decoder_LSTM</h3>
<blockquote class="blockquote">
<pre><code> Decoder_LSTM (in_features, out_features)</code></pre>
</blockquote>
<p>Decode with LSTM</p>
<hr>
</section>
<section id="decoder_linear" class="level3">
<h3 class="anchored" data-anchor-id="decoder_linear">Decoder_Linear</h3>
<blockquote class="blockquote">
<pre><code> Decoder_Linear (in_features, out_features)</code></pre>
</blockquote>
<p>Decode w linear NN</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>