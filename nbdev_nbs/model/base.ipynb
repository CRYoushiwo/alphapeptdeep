{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from alphadeep._settings import global_settings\n",
    "torch.set_num_threads(global_settings['thread_num'])\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from typing import IO, Tuple, List, Union\n",
    "from alphabase.yaml_utils import save_yaml\n",
    "from alphabase.peptide.precursor import is_precursor_sorted\n",
    "\n",
    "from alphadeep._settings import model_const\n",
    "\n",
    "from alphadeep.model.building_block import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelImplBase\n",
    "The base model for operations of all models, it does not contains the model (torch.nn.Module), but just provides some common APIs, including `load()` to load models, `save()` to save modles, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ModelImplBase(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = None\n",
    "        if 'GPU' in kwargs:\n",
    "            self.use_GPU(kwargs['GPU'])\n",
    "        else:\n",
    "            self.use_GPU(True)\n",
    "\n",
    "    def use_GPU(self, GPU=True):\n",
    "        if not torch.cuda.is_available():\n",
    "            GPU=False\n",
    "        self.device = torch.device('cuda' if GPU else 'cpu')\n",
    "        if self.model:\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def _init_for_train(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_func = torch.nn.L1Loss()\n",
    "\n",
    "    def build_from_py_codes(self,\n",
    "        model_code_file:str,\n",
    "        code_file_in_zip:str=None,\n",
    "        lr = 0.001,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if model_code_file.lower().endswith('.zip'):\n",
    "            with ZipFile(model_code_file, 'r') as model_zip:\n",
    "                with model_zip.open(code_file_in_zip) as f:\n",
    "                    codes = f.read()\n",
    "        else:\n",
    "            with open(model_code_file, 'r') as f:\n",
    "                codes = f.read()\n",
    "        codes = compile(\n",
    "            codes, \n",
    "            filename='model_file_py',\n",
    "            mode='exec'\n",
    "        )\n",
    "        exec(codes) #codes must contains torch model codes 'class Model(...'\n",
    "        self.model = Model(**kwargs)\n",
    "        self.model_params = kwargs\n",
    "        self.model.to(self.device)\n",
    "        self._init_for_train(lr)\n",
    "\n",
    "    def build(self,\n",
    "        model_class: torch.nn.Module,\n",
    "        lr = 0.001,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model = model_class(**kwargs)\n",
    "        self.model_params = kwargs\n",
    "        self.model.to(self.device)\n",
    "        self._init_for_train(lr)\n",
    "\n",
    "    def get_parameter_num(self):\n",
    "        return np.sum([p.numel() for p in self.model.parameters()])\n",
    "\n",
    "    def _save_codes(self, save_as):\n",
    "        import inspect\n",
    "        code = '''import torch\\nimport alphadeep.model.base as model_base\\n'''\n",
    "        class_code = inspect.getsource(self.model.__class__)\n",
    "        code += 'class Model' + class_code[class_code.find('('):]\n",
    "        with open(save_as, 'w') as f:\n",
    "            f.write(code)\n",
    "\n",
    "    def save(self, save_as):\n",
    "        dir = os.path.dirname(save_as)\n",
    "        if not dir: dir = './'\n",
    "        if not os.path.exists(dir): os.makedirs(dir)\n",
    "        torch.save(self.model.state_dict(), save_as)\n",
    "        with open(save_as+'.txt','w') as f: f.write(str(self.model))\n",
    "        save_yaml(save_as+'.model_const.yaml', model_const)\n",
    "        self._save_codes(save_as+'.model.py')\n",
    "        save_yaml(save_as+'.param.yaml', self.model_params)\n",
    "\n",
    "    def _load_model_file(self, stream):\n",
    "        (\n",
    "            missing_keys, unexpect_keys \n",
    "        ) = self.model.load_state_dict(torch.load(\n",
    "            stream, map_location=self.device),\n",
    "            strict=False\n",
    "        )\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        model_file: Tuple[str, IO],\n",
    "        model_path_in_zip: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if isinstance(model_file, str):\n",
    "            # We may release all models (msms, rt, ccs, ...) in a single zip file\n",
    "            if model_file.lower().endswith('.zip'):\n",
    "                with ZipFile(model_file) as model_zip:\n",
    "                    with model_zip.open(model_path_in_zip,'r') as pt_file:\n",
    "                        self._load_model_file(pt_file)\n",
    "            else:\n",
    "                with open(model_file,'rb') as pt_file:\n",
    "                    self._load_model_file(pt_file)\n",
    "        else:\n",
    "            self._load_model_file(model_file)\n",
    "\n",
    "    def _train_one_batch(\n",
    "        self, \n",
    "        targets:Union[torch.Tensor,List[torch.Tensor]], \n",
    "        *features,\n",
    "    ):\n",
    "        self.optimizer.zero_grad()\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        if isinstance(targets, list):\n",
    "            # predicts must be a list or tuple as well\n",
    "            cost = self.loss_func(\n",
    "                predicts,\n",
    "                [t.to(self.device) for t in targets]\n",
    "            )\n",
    "        else:\n",
    "            cost = self.loss_func(predicts, targets.to(self.device))\n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "        return cost.item()\n",
    "\n",
    "    def _predict_one_batch(self,\n",
    "        *features\n",
    "    ):\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        if isinstance(predicts, torch.Tensor):\n",
    "            return predicts.cpu().detach().numpy()\n",
    "        else:\n",
    "            return [p.cpu().detach().numpy() for p in predicts]\n",
    "\n",
    "    def _get_targets_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        nAA=None, **kwargs,\n",
    "    )->Union[torch.Tensor,List]:\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_targets_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _get_features_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        nAA, **kwargs,\n",
    "    )->Tuple[torch.Tensor]:\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_features_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _prepare_predict_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kwargs\n",
    "    ):\n",
    "        '''\n",
    "        This method must create a `self.predict_df` dataframe.\n",
    "        '''\n",
    "        self.predict_df = pd.DataFrame()\n",
    "\n",
    "    def _prepare_train_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kwargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def _set_batch_predict_data(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        predicts:Union[torch.Tensor, List],\n",
    "        **kwargs\n",
    "    ):\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _set_batch_predict_data_df() method'\n",
    "        )\n",
    "\n",
    "    def train(self,\n",
    "        precursor_df: pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024, \n",
    "        epoch=20, \n",
    "        verbose=False,\n",
    "        verbose_each_epoch=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if 'nAA' not in precursor_df.columns:\n",
    "            precursor_df['nAA'] = precursor_df.sequence.str.len()\n",
    "        self._prepare_train_data_df(precursor_df, **kwargs)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            batch_cost = []\n",
    "            _grouped = list(precursor_df.sample(frac=1).groupby('nAA'))\n",
    "            rnd_nAA = np.random.permutation(len(_grouped))\n",
    "            if verbose_each_epoch:\n",
    "                batch_tqdm = tqdm(rnd_nAA)\n",
    "            else:\n",
    "                batch_tqdm = rnd_nAA\n",
    "            for i_group in batch_tqdm:\n",
    "                nAA, df_group = _grouped[i_group]\n",
    "                df_group = df_group.reset_index(drop=True)\n",
    "                for i in range(0, len(df_group), batch_size):\n",
    "                    batch_end = i+batch_size-1 # DataFrame.loc[start:end] inlcudes the end\n",
    "\n",
    "                    batch_df = df_group.loc[i:batch_end,:]\n",
    "                    targets = self._get_targets_from_batch_df(batch_df,nAA=nAA,**kwargs)\n",
    "                    features = self._get_features_from_batch_df(batch_df,nAA=nAA,**kwargs)\n",
    "                    \n",
    "                    cost = self._train_one_batch(\n",
    "                        targets, \n",
    "                        *features,\n",
    "                    )\n",
    "                    batch_cost.append(cost)\n",
    "                if verbose_each_epoch:\n",
    "                    batch_tqdm.set_description(\n",
    "                        f'Epoch={epoch+1}, nAA={nAA}, Batch={len(batch_cost)}, Loss={cost:.4f}'\n",
    "                    )\n",
    "            if verbose: print(f'[Training] Epoch={epoch+1}, Mean Loss={np.mean(batch_cost)}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def _check_predict_in_order(self, precursor_df:pd.DataFrame):\n",
    "        if is_precursor_sorted(precursor_df):\n",
    "            self._predict_in_order = True\n",
    "        else:\n",
    "            self._predict_in_order = False\n",
    "\n",
    "    def predict(self,\n",
    "        precursor_df:pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024,\n",
    "        verbose=False,\n",
    "        **kwargs\n",
    "    )->pd.DataFrame:\n",
    "        if 'nAA' not in precursor_df.columns:\n",
    "            precursor_df['nAA'] = precursor_df.sequence.str.len()\n",
    "            precursor_df.sort_values('nAA', inplace=True)\n",
    "            precursor_df.reset_index(drop=True,inplace=True)\n",
    "        self._check_predict_in_order(precursor_df)\n",
    "        self._prepare_predict_data_df(precursor_df,**kwargs)\n",
    "        self.model.eval()\n",
    "\n",
    "        _grouped = precursor_df.groupby('nAA')\n",
    "        if verbose:\n",
    "            batch_tqdm = tqdm(_grouped)\n",
    "        else:\n",
    "            batch_tqdm = _grouped\n",
    "        with torch.no_grad():\n",
    "            for nAA, df_group in batch_tqdm:\n",
    "                for i in range(0, len(df_group), batch_size):\n",
    "                    batch_end = i+batch_size\n",
    "                    \n",
    "                    batch_df = df_group.iloc[i:batch_end,:]\n",
    "\n",
    "                    features = self._get_features_from_batch_df(\n",
    "                        batch_df, nAA, **kwargs\n",
    "                    )\n",
    "\n",
    "                    predicts = self._predict_one_batch(*features)\n",
    "\n",
    "                    self._set_batch_predict_data(\n",
    "                        batch_df, predicts, \n",
    "                        **kwargs\n",
    "                    )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
