{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#default_exp model.base"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from typing import IO, Type, Tuple"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ModelImplBase\n",
    "The base model for operations of all models, it does not contains the model (torch.nn.Module), but just provides some common APIs, including `load()` to load models, `save()` to save modles, ..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#export\n",
    "class ModelImplBase(object):\n",
    "    def __init__(self, *args, **kargs):\n",
    "        if 'GPU' in kargs:\n",
    "            self.use_GPU(kargs['GPU'])\n",
    "        else:\n",
    "            self.use_GPU(True)\n",
    "\n",
    "    def use_GPU(self, GPU=True):\n",
    "        if GPU and not torch.cuda.is_available():\n",
    "            GPU=False\n",
    "        self.device = torch.device('cuda' if GPU else 'cpu')\n",
    "\n",
    "    def init_train(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_func = torch.nn.L1Loss()\n",
    "\n",
    "    def build(self,\n",
    "        model_class: Type[torch.nn.Module],\n",
    "        lr = 0.001,\n",
    "        *args, **kargs\n",
    "    ):\n",
    "        self.model = model_class(*args, **kargs)\n",
    "        self.model.to(self.device)\n",
    "        self.init_train(lr)\n",
    "\n",
    "    def get_parameter_num(self):\n",
    "        return np.sum([p.numel() for p in self.model.parameters()])\n",
    "\n",
    "    def save(self, save_as):\n",
    "        dir = os.path.dirname(save_as)\n",
    "        if not dir: dir = './'\n",
    "        if not os.path.exists(dir): os.makedirs(dir)\n",
    "        torch.save(self.model.state_dict(), save_as)\n",
    "        with open(save_as+'.txt','w') as f: f.write(str(self.model))\n",
    "\n",
    "    def _load_model_file(self, stream):\n",
    "        self.model.load_state_dict(torch.load(\n",
    "            stream, map_location=self.device)\n",
    "        )\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        model_file: Tuple[str, IO],\n",
    "        model_name_in_zip: str = None,\n",
    "        *args, **kargs\n",
    "    ):\n",
    "        if isinstance(model_file, str):\n",
    "            # We may release all models (msms, rt, ccs, ...) in a single zip file\n",
    "            if model_file.lower().endswith('.zip'):\n",
    "                with ZipFile(model_file, 'rb') as model_zip:\n",
    "                    with model_zip.open(model_name_in_zip) as pt_file:\n",
    "                        self._load_model_file(pt_file)\n",
    "            else:\n",
    "                with open(model_file,'rb') as pt_file:\n",
    "                    self._load_model_file(pt_file)\n",
    "        else:\n",
    "            self._load_model_file(model_file)\n",
    "\n",
    "    def _train_one_batch(\n",
    "        self, \n",
    "        targets:torch.Tensor, \n",
    "        *features\n",
    "    ):\n",
    "        self.optimizer.zero_grad()\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        cost = self.loss_func(predicts, targets.to(self.device))\n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "        return cost\n",
    "\n",
    "    def _predict_one_batch(self,\n",
    "        *features\n",
    "    ):\n",
    "        return self.model(*[fea.to(self.device) for fea in features])\n",
    "\n",
    "    def train(self, batch_size=1024, epoch=20, *args, **kargs):\n",
    "        raise NotImplementedError('train() function is not finished yet')\n",
    "\n",
    "    def predict(self, batch_size=1024, *args, **kargs):\n",
    "        raise NotImplementedError('predict() function is not finished yet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Here we provide some basic torch sub-models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#export\n",
    "def zero_param(*shape):\n",
    "    return torch.nn.Parameter(torch.zeros(shape), requires_grad=False)\n",
    "\n",
    "def xavier_param(*shape):\n",
    "    x = torch.nn.Parameter(torch.empty(shape), requires_grad=False)\n",
    "    torch.nn.init.xavier_uniform_(x)\n",
    "    return x\n",
    "\n",
    "init_state = xavier_param\n",
    "\n",
    "def aa_embedding(embedding_size):\n",
    "    return torch.nn.Embedding(27, embedding_size, padding_idx=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from alphadeep.model.featurize import parse_aa_indices\n",
    "sequence = 'ACDEFGIK'\n",
    "\n",
    "embedding_hidden = 4\n",
    "embedding = aa_embedding(embedding_hidden)\n",
    "x = embedding(torch.LongTensor(parse_aa_indices([sequence])))\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.5638,  0.8596, -0.0549, -0.0189],\n",
       "         [ 0.9118,  1.0247,  3.3288,  0.7046],\n",
       "         [-0.3026,  0.4432,  0.0145,  0.8551],\n",
       "         [-1.5997,  2.1119,  0.9526, -0.5363],\n",
       "         [ 0.1958,  0.9696, -0.3995, -0.9213],\n",
       "         [-1.0171,  0.9094,  1.2428, -1.2176],\n",
       "         [-0.4703, -1.4687,  0.7420, -0.6106],\n",
       "         [-1.3442,  0.8077,  1.5787, -0.2008],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`SeqCNN` or TextCNN extracts sequence features using `nn.Conv1D` with different kernel sizes (3,5,7), and then concatenates the outputs of these Conv1Ds."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#export\n",
    "class SeqCNN(torch.nn.Module):\n",
    "    def __init__(self, embedding_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_short = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=3, padding=1\n",
    "        )\n",
    "        self.cnn_medium = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=5, padding=2\n",
    "        )\n",
    "        self.cnn_long = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=7, padding=3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x1 = self.cnn_short(x)\n",
    "        x2 = self.cnn_medium(x)\n",
    "        x3 = self.cnn_long(x)\n",
    "        return torch.cat((x, x1, x2, x3), dim=1).transpose(1,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`SeqInput` takes embedded sequences as the input, processes inputs using `SeqCNN`, and outputs RNN results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#export\n",
    "class SeqLSTM(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden, \n",
    "                 rnn_layer=2, bidirectional=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_h0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional,\n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn_c0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional,\n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = rnn_layer,\n",
    "            batch_first = True,\n",
    "            bidirectional = bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        c0 = self.rnn_c0.repeat(1, x.size(0), 1)\n",
    "        x, _ = self.rnn(x, (h0,c0))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SeqGRU(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden, \n",
    "                 rnn_layer=2, bidirectional=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_h0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional, \n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = rnn_layer,\n",
    "            batch_first = True,\n",
    "            bidirectional = bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#export\n",
    "class SeqAttentionSum(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 1, bias=False),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.attn(x)\n",
    "        return torch.sum(torch.mul(x, attn), dim=1)\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import torch\n",
    "x = [[1,2,3,4,5,6],[1,2,3,1,2,3]]\n",
    "x = torch.LongTensor(x)\n",
    "x = torch.nn.functional.one_hot(x, 7).float()\n",
    "attn = SeqAttentionSum(7)\n",
    "attn(x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1061, 0.1317, 0.1841, 0.1706, 0.1997, 0.2078],\n",
       "        [0.0000, 0.2515, 0.3121, 0.4363, 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#export\n",
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 128),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(128, 32),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(32, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test these basic models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SeqEncoder(torch.nn.Module):\n",
    "    def __init__(self, embedding_hidden, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.input_cnn = SeqCNN(embedding_hidden)\n",
    "        self.input_nn = SeqLSTM(embedding_hidden*4, embedding_hidden*4, rnn_layer=2)\n",
    "        self.attn_sum = SeqAttentionSum(embedding_hidden*8) #4 for MultiScaleCNN output, and 2 for BiRNN output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_cnn(x)\n",
    "        x = self.input_nn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.attn_sum(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder = SeqEncoder(embedding_hidden)\n",
    "code = encoder(x)\n",
    "code"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.3658,  1.1804,  2.5408, -1.0424,  1.7857, -0.4062,  0.4979,  0.8366,\n",
       "          0.4606,  1.4251, -0.3487, -0.4141, -1.8821, -1.1386,  1.0718,  0.4254,\n",
       "         -0.4744, -1.6219, -0.7583, -1.6310,  1.3904,  0.4034,  0.6103, -1.9193,\n",
       "          0.5267, -1.7338, -0.0106, -0.2590, -0.6797, -0.2902, -0.2994,  1.3211]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#hide\n",
    "encoder"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SeqEncoder(\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (input_cnn): SeqCNN(\n",
       "    (cnn_short): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (cnn_medium): Conv1d(4, 4, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (cnn_long): Conv1d(4, 4, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  )\n",
       "  (input_nn): SeqLSTM(\n",
       "    (rnn): LSTM(16, 16, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attn_sum): SeqAttentionSum(\n",
       "    (pos_weight): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=1, bias=False)\n",
       "      (1): Softmax(dim=2)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SeqDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = 128\n",
    "        self.rnn_h0 = init_state(1, 1, hidden)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "        )\n",
    "\n",
    "        self.output_nn = torch.nn.Linear(\n",
    "            hidden, out_features, bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        x, h = self.rnn(x, h0)\n",
    "        x = self.output_nn(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decoder = SeqDecoder(embedding_hidden*8, 2)\n",
    "\n",
    "decode = decoder(code.unsqueeze(1).repeat(1, len(sequence), 1))\n",
    "decode"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-0.1205, -0.0147],\n",
       "         [-0.1780, -0.0156],\n",
       "         [-0.2151, -0.0192],\n",
       "         [-0.2380, -0.0241],\n",
       "         [-0.2519, -0.0286],\n",
       "         [-0.2603, -0.0321],\n",
       "         [-0.2655, -0.0346],\n",
       "         [-0.2688, -0.0362]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "308d4ed10f4e428ae4be641589bd1f5718a1c1123bc653e4079456a6f5f94028"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}