{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from typing import IO, Tuple, List, Union\n",
    "from alphabase.yaml_utils import save_yaml\n",
    "from alphadeep._settings import model_const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelImplBase\n",
    "The base model for operations of all models, it does not contains the model (torch.nn.Module), but just provides some common APIs, including `load()` to load models, `save()` to save modles, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ModelImplBase(object):\n",
    "    def __init__(self, *args, **kargs):\n",
    "        if 'GPU' in kargs:\n",
    "            self.use_GPU(kargs['GPU'])\n",
    "        else:\n",
    "            self.use_GPU(True)\n",
    "\n",
    "    def use_GPU(self, GPU=True):\n",
    "        if GPU and not torch.cuda.is_available():\n",
    "            GPU=False\n",
    "        self.device = torch.device('cuda' if GPU else 'cpu')\n",
    "\n",
    "    def init_train(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_func = torch.nn.L1Loss()\n",
    "\n",
    "    def build(self,\n",
    "        model_class: torch.nn.Module,\n",
    "        lr = 0.001,\n",
    "        **kargs\n",
    "    ):\n",
    "        self.model = model_class(**kargs)\n",
    "        self.model.to(self.device)\n",
    "        self.init_train(lr)\n",
    "\n",
    "    def get_parameter_num(self):\n",
    "        return np.sum([p.numel() for p in self.model.parameters()])\n",
    "\n",
    "    def save(self, save_as):\n",
    "        dir = os.path.dirname(save_as)\n",
    "        if not dir: dir = './'\n",
    "        if not os.path.exists(dir): os.makedirs(dir)\n",
    "        torch.save(self.model.state_dict(), save_as)\n",
    "        with open(save_as+'.txt','w') as f: f.write(str(self.model))\n",
    "        save_yaml(save_as+'.model_const.yaml', model_const)\n",
    "\n",
    "    def _load_model_file(self, stream):\n",
    "        self.model.load_state_dict(torch.load(\n",
    "            stream, map_location=self.device)\n",
    "        )\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        model_file: Tuple[str, IO],\n",
    "        model_name_in_zip: str = None,\n",
    "        **kargs\n",
    "    ):\n",
    "        if isinstance(model_file, str):\n",
    "            # We may release all models (msms, rt, ccs, ...) in a single zip file\n",
    "            if model_file.lower().endswith('.zip'):\n",
    "                with ZipFile(model_file, 'rb') as model_zip:\n",
    "                    with model_zip.open(model_name_in_zip) as pt_file:\n",
    "                        self._load_model_file(pt_file)\n",
    "            else:\n",
    "                with open(model_file,'rb') as pt_file:\n",
    "                    self._load_model_file(pt_file)\n",
    "        else:\n",
    "            self._load_model_file(model_file)\n",
    "\n",
    "    def _train_one_batch(\n",
    "        self, \n",
    "        targets:Union[torch.Tensor,List[torch.Tensor]], \n",
    "        *features,\n",
    "    ):\n",
    "        self.optimizer.zero_grad()\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        if isinstance(targets, list):\n",
    "            # predicts must be a list or tuple as well\n",
    "            cost = self.loss_func(\n",
    "                predicts,\n",
    "                [t.to(self.device) for t in targets]\n",
    "            )\n",
    "        else:\n",
    "            cost = self.loss_func(predicts, targets.to(self.device))\n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "        return cost.item()\n",
    "\n",
    "    def _predict_one_batch(self,\n",
    "        *features\n",
    "    ):\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        if isinstance(predicts, torch.Tensor):\n",
    "            return predicts.cpu().detach().numpy()\n",
    "        else:\n",
    "            return [p.cpu().detach().numpy() for p in predicts]\n",
    "\n",
    "    def _get_targets_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        nAA, **kargs,\n",
    "    )->Union[torch.Tensor,List]:\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_targets_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _get_features_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        nAA, **kargs,\n",
    "    )->Tuple[torch.Tensor]:\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_features_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _prepare_predict_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kargs\n",
    "    ):\n",
    "        self.predict_df = pd.DataFrame()\n",
    "\n",
    "    def _prepare_train_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def _set_batch_predict_data(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        predicts:Union[torch.Tensor, List],\n",
    "        **kargs\n",
    "    ):\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _set_batch_predict_data_df() method'\n",
    "        )\n",
    "\n",
    "    def train(self,\n",
    "        precursor_df: pd.DataFrame,\n",
    "        batch_size=1024, \n",
    "        epoch=20, \n",
    "        verbose=False,\n",
    "        verbose_each_epoch=True,\n",
    "        **kargs\n",
    "    ):\n",
    "        self._prepare_train_data_df(precursor_df, **kargs)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            batch_cost = []\n",
    "            _grouped = list(precursor_df.sample(frac=1).groupby('nAA'))\n",
    "            rnd_nAA = np.random.permutation(len(_grouped))\n",
    "            if verbose_each_epoch:\n",
    "                batch_tqdm = tqdm(rnd_nAA)\n",
    "            else:\n",
    "                batch_tqdm = rnd_nAA\n",
    "            for i_group in batch_tqdm:\n",
    "                nAA, df_group = _grouped[i_group]\n",
    "                df_group = df_group.reset_index(drop=True)\n",
    "                for i in range(0, len(df_group), batch_size):\n",
    "                    batch_end = i+batch_size-1 # DataFrame.loc[start:end] inlcudes the end\n",
    "\n",
    "                    batch_df = df_group.loc[i:batch_end,:]\n",
    "                    targets = self._get_targets_from_batch_df(batch_df,nAA,**kargs)\n",
    "                    features = self._get_features_from_batch_df(batch_df,nAA,**kargs)\n",
    "                    \n",
    "                    cost = self._train_one_batch(\n",
    "                        targets, \n",
    "                        *features,\n",
    "                    )\n",
    "                    batch_cost.append(cost)\n",
    "                if verbose_each_epoch:\n",
    "                    batch_tqdm.set_description(\n",
    "                        f'Epoch={epoch+1}, nAA={nAA}, Batch={len(batch_cost)}, Loss={cost:.4f}'\n",
    "                    )\n",
    "            if verbose: print(f'[Training] Epoch={epoch+1}, Mean Loss={np.mean(batch_cost)}')\n",
    "\n",
    "    def predict(self,\n",
    "        precursor_df:pd.DataFrame,\n",
    "        batch_size=1024,\n",
    "        verbose=False,**kargs\n",
    "    )->pd.DataFrame:\n",
    "        self._prepare_predict_data_df(precursor_df,**kargs)\n",
    "        self.model.eval()\n",
    "\n",
    "        _grouped = precursor_df.groupby('nAA')\n",
    "        if verbose:\n",
    "            batch_tqdm = tqdm(_grouped)\n",
    "        else:\n",
    "            batch_tqdm = _grouped\n",
    "\n",
    "        for nAA, df_group in batch_tqdm:\n",
    "            for i in range(0, len(df_group), batch_size):\n",
    "                batch_end = i+batch_size # DataFrame.loc[start:end] inlcudes the end\n",
    "                \n",
    "                batch_df = df_group.iloc[i:batch_end,:]\n",
    "\n",
    "                features = self._get_features_from_batch_df(\n",
    "                    batch_df, nAA, **kargs\n",
    "                )\n",
    "\n",
    "                predicts = self._predict_one_batch(*features)\n",
    "\n",
    "                self._set_batch_predict_data(\n",
    "                    batch_df, predicts, \n",
    "                    **kargs\n",
    "                )\n",
    "\n",
    "        return self.predict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we provide some basic torch sub-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def zero_param(*shape):\n",
    "    return torch.nn.Parameter(torch.zeros(shape), requires_grad=False)\n",
    "\n",
    "def xavier_param(*shape):\n",
    "    x = torch.nn.Parameter(torch.empty(shape), requires_grad=False)\n",
    "    torch.nn.init.xavier_uniform_(x)\n",
    "    return x\n",
    "\n",
    "init_state = xavier_param\n",
    "\n",
    "def aa_embedding(embedding_size):\n",
    "    return torch.nn.Embedding(27, embedding_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0731, -0.8206, -0.7814, -1.1839],\n",
       "         [ 1.1793, -1.0558, -1.3909, -0.4541],\n",
       "         [-1.0410, -1.2317, -0.2053,  0.8490],\n",
       "         [ 0.3735,  1.9244,  1.7362,  0.7252],\n",
       "         [-1.4987,  0.8496,  0.2130, -0.6962],\n",
       "         [-1.5180, -1.5192, -0.4297,  0.7549],\n",
       "         [-0.3256, -1.3425,  0.1349, -0.3748],\n",
       "         [ 1.1087, -1.9703,  0.4775,  1.5344],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alphadeep.model.featurize import parse_aa_indices\n",
    "sequence = 'ACDEFGIK'\n",
    "\n",
    "embedding_hidden = 4\n",
    "embedding = aa_embedding(embedding_hidden)\n",
    "x = embedding(torch.LongTensor(parse_aa_indices([sequence])))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SeqCNN` or TextCNN extracts sequence features using `nn.Conv1D` with different kernel sizes (3,5,7), and then concatenates the outputs of these Conv1Ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqCNN(torch.nn.Module):\n",
    "    def __init__(self, embedding_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_short = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=3, padding=1\n",
    "        )\n",
    "        self.cnn_medium = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=5, padding=2\n",
    "        )\n",
    "        self.cnn_long = torch.nn.Conv1d(\n",
    "            embedding_hidden, embedding_hidden,\n",
    "            kernel_size=7, padding=3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x1 = self.cnn_short(x)\n",
    "        x2 = self.cnn_medium(x)\n",
    "        x3 = self.cnn_long(x)\n",
    "        return torch.cat((x, x1, x2, x3), dim=1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SeqInput` takes embedded sequences as the input, processes inputs using `SeqCNN`, and outputs RNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqLSTM(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 rnn_layer=2, bidirectional=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden = out_features//2\n",
    "        else:\n",
    "            hidden = out_features\n",
    "\n",
    "        self.rnn_h0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional,\n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn_c0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional,\n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = rnn_layer,\n",
    "            batch_first = True,\n",
    "            bidirectional = bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        c0 = self.rnn_c0.repeat(1, x.size(0), 1)\n",
    "        x, _ = self.rnn(x, (h0,c0))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SeqGRU(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 rnn_layer=2, bidirectional=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden = out_features//2\n",
    "        else:\n",
    "            hidden = out_features\n",
    "        \n",
    "        self.rnn_h0 = init_state(\n",
    "            rnn_layer+rnn_layer*bidirectional, \n",
    "            1, hidden\n",
    "        )\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = rnn_layer,\n",
    "            batch_first = True,\n",
    "            bidirectional = bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqAttentionSum(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 1, bias=False),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.attn(x)\n",
    "        return torch.sum(torch.mul(x, attn), dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1218, 0.1746, 0.1967, 0.1767, 0.1420, 0.1882],\n",
       "        [0.0000, 0.2470, 0.3541, 0.3989, 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = [[1,2,3,4,5,6],[1,2,3,1,2,3]]\n",
    "x = torch.LongTensor(x)\n",
    "x = torch.nn.functional.one_hot(x, 7).float()\n",
    "attn = SeqAttentionSum(7)\n",
    "attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 64),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(64, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test these basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, rnn_layer=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.input_cnn = SeqCNN(in_features)\n",
    "        self.hidden_nn = SeqLSTM(in_features*4, out_features, rnn_layer=rnn_layer) #4 for MultiScaleCNN output\n",
    "        self.attn_sum = SeqAttentionSum(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_cnn(x)\n",
    "        x = self.hidden_nn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.attn_sum(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0180, 0.0596, 0.1790, 0.0914],\n",
       "        [0.0205, 0.0315, 0.0975, 0.0756]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,3,4,5,6],[1,2,3,1,2,3]]\n",
    "x = torch.LongTensor(x)\n",
    "x = torch.nn.functional.one_hot(x, 7).float()\n",
    "embedding_hidden=7\n",
    "encoder = SeqEncoder(7,4)\n",
    "code = encoder(x)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqEncoder(\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (input_cnn): SeqCNN(\n",
       "    (cnn_short): Conv1d(7, 7, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (cnn_medium): Conv1d(7, 7, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (cnn_long): Conv1d(7, 7, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  )\n",
       "  (hidden_nn): SeqLSTM(\n",
       "    (rnn): LSTM(28, 2, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attn_sum): SeqAttentionSum(\n",
       "    (attn): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=1, bias=False)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = 128\n",
    "        self.rnn_h0 = init_state(1, 1, hidden)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = in_features,\n",
    "            hidden_size = hidden,\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "        )\n",
    "\n",
    "        self.output_nn = torch.nn.Linear(\n",
    "            hidden, out_features, bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = self.rnn_h0.repeat(1, x.size(0), 1)\n",
    "        x, h = self.rnn(x, h0)\n",
    "        x = self.output_nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0173, -0.0680],\n",
       "         [-0.0073, -0.0656],\n",
       "         [-0.0012, -0.0649],\n",
       "         [ 0.0024, -0.0647],\n",
       "         [ 0.0046, -0.0648],\n",
       "         [ 0.0059, -0.0649],\n",
       "         [ 0.0067, -0.0650],\n",
       "         [ 0.0072, -0.0651]],\n",
       "\n",
       "        [[-0.0146, -0.0660],\n",
       "         [-0.0034, -0.0627],\n",
       "         [ 0.0033, -0.0614],\n",
       "         [ 0.0071, -0.0611],\n",
       "         [ 0.0094, -0.0610],\n",
       "         [ 0.0107, -0.0611],\n",
       "         [ 0.0115, -0.0611],\n",
       "         [ 0.0119, -0.0612]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = SeqDecoder(4, 2)\n",
    "\n",
    "decode = decoder(code.unsqueeze(1).repeat(1, len(sequence), 1))\n",
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47571f5fdb57242938b1c768688b8dffd916e56712176e488a19312fe26ffb57"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
