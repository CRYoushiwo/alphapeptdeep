{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#default_exp model.CCS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#export\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from alphadeep.model.featurize import \\\n",
    "    parse_aa_indices, \\\n",
    "    get_batch_mod_feature\n",
    "\n",
    "from alphadeep._settings import \\\n",
    "    global_settings as settings, \\\n",
    "    const_settings\n",
    "\n",
    "import alphadeep.model.base as model_base\n",
    "\n",
    "class TorchModelCCS(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        mod_feature_size,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        BiRNN = True\n",
    "        self.aa_embedding_size = 27\n",
    "        hidden=128\n",
    "        hidden_rnn_layer=2\n",
    "        \n",
    "        # ins_nce_embed_size = conf.max_instrument_num+1\n",
    "        # self.instrument_nce_embed = torch.nn.Identity()\n",
    "        \n",
    "        output_hidden_size = hidden*(2 if BiRNN else 1)\n",
    "        \n",
    "        # mod_embed_size = 8\n",
    "        # self.mod_embed_weights = torch.nn.Parameter(\n",
    "            # torch.empty(mod_size, mod_embed_size), \n",
    "            # requires_grad = True\n",
    "        # )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.input = model_base.SeqLSTM(\n",
    "            self.aa_embedding_size+mod_feature_size+1, \n",
    "            hidden, rnn_layer=1, \n",
    "            bidirectional=BiRNN\n",
    "        )\n",
    "        \n",
    "        self.hidden = model_base.SeqLSTM(\n",
    "            output_hidden_size+1, \n",
    "            hidden, rnn_layer=hidden_rnn_layer, \n",
    "            bidirectional=BiRNN\n",
    "        )\n",
    "        \n",
    "        self.output = model_base.LinearDecoder(\n",
    "            output_hidden_size*2,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "        aa_indices, \n",
    "        mod_x, \n",
    "        charges:torch.Tensor,\n",
    "    ):\n",
    "        aa_x = torch.nn.functional.one_hot(aa_indices, self.aa_embedding_size)\n",
    "        charges = charges.unsqueeze(2)\n",
    "\n",
    "        x = torch.cat((aa_x, mod_x, charges), 2)\n",
    "        x = self.input(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.cat((x, charges), 2)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.cat((x[:,0,:],x[:,-1,:]),1)\n",
    "\n",
    "        return self.output(x).squeeze(1)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alphabase'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d85926984e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malphabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeptide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[0;32mimport\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0minit_fragment_by_precursor_dataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mset_sliced_fragment_dataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alphabase'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "308d4ed10f4e428ae4be641589bd1f5718a1c1123bc653e4079456a6f5f94028"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}