{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#default_exp model.RT"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#export\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from alphadeep.model.featurize import \\\n",
    "    parse_aa_indices, parse_instrument_indices, \\\n",
    "    get_batch_mod_feature\n",
    "\n",
    "from alphadeep._settings import \\\n",
    "    global_settings as settings, \\\n",
    "    const_settings\n",
    "\n",
    "import alphadeep.model.base as model_base\n",
    "\n",
    "class ModelCCS(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        mod_feature_size,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        BiRNN = True\n",
    "        self.aa_embedding_size = 27\n",
    "        hidden=256\n",
    "        \n",
    "        # ins_nce_embed_size = conf.max_instrument_num+1\n",
    "        # self.instrument_nce_embed = torch.nn.Identity()\n",
    "        \n",
    "        output_hidden_size = hidden*(2 if BiRNN else 1)\n",
    "        \n",
    "        # mod_embed_size = 8\n",
    "        # self.mod_embed_weights = torch.nn.Parameter(\n",
    "            # torch.empty(mod_size, mod_embed_size), \n",
    "            # requires_grad = True\n",
    "        # )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.input = model_base.SeqLSTM(\n",
    "            self.aa_embedding_size+mod_feature_size, \n",
    "            hidden, rnn_layer=1, \n",
    "            bidirectional=BiRNN\n",
    "        )\n",
    "        \n",
    "        self.hidden = model_base.SeqLSTM(\n",
    "            output_hidden_size, \n",
    "            hidden, rnn_layer=1, \n",
    "            bidirectional=BiRNN\n",
    "        )\n",
    "        \n",
    "        self.output = model_base.LinearDecoder(\n",
    "            output_hidden_size*2,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "        aa_indices, \n",
    "        mod_x,\n",
    "    ):\n",
    "        aa_x = torch.nn.functional.one_hot(\n",
    "            aa_indices, self.aa_embedding_size\n",
    "        )\n",
    "\n",
    "        x = torch.cat((aa_x, mod_x), 2)\n",
    "        x = self.input(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.cat((x[:,0,:],x[:,-1,:]),1)\n",
    "\n",
    "        return self.output(x).squeeze(1)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}