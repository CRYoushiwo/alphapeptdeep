{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.model_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from typing import IO, Tuple, List, Union\n",
    "from alphabase.yaml_utils import save_yaml\n",
    "from alphabase.peptide.precursor import is_precursor_sorted\n",
    "\n",
    "from peptdeep.settings import model_const\n",
    "from peptdeep.utils import logging\n",
    "\n",
    "from peptdeep.model.building_block import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# copied from huggingface\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, \n",
    "    num_training_steps, num_cycles=0.5, \n",
    "    last_epoch=-1\n",
    "):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function between 0 and `pi * cycles` after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / max(1, num_warmup_steps)\n",
    "        progress = float(\n",
    "            current_step - num_warmup_steps\n",
    "        ) / max(1, num_training_steps - num_warmup_steps)\n",
    "        return max(0.0, 0.5 * (\n",
    "            1.0 + np.cos(np.pi * num_cycles * 2.0 * progress)\n",
    "        ))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeptideModelInterfaceBase\n",
    "This base model defines interfaces for all deep learning (pytorch) models of peptides. It does not contains the model (torch.nn.Module), but just provides some common APIs, including `load()` to load models, `save()` to save modles, `build()`, `train()`, `predict()`... That means, we do not need to re-define these methods in the sub-classes for most of the cases. We just need to re-implement `_get_features_from_batch_df()`, `_get_targets_from_batch_df()`, `_prepare_predict_data_df()`. `PeptideModelInterfaceBase` will adapt training and predicting procedures. The base class will automatically empty the GPU cache at the end of `train()` and `predict()` to save GPU memories.\n",
    "\n",
    "If we would like to design a new model for peptides with different purposes, for exapme RT prediction, we need to:\n",
    "- Design the pytorch model (`class TorchRTPrediction(torch.nn.Module):...`)\n",
    "- Design the sub-class inherited from PeptideModelInterfaceBase (`class RTPredictionModel(PeptideModelInterfaceBase):...`)\n",
    "- Re-implement `def _get_features_from_batch_df(self, batch_df, nAA:int): ... return torch.LongTensor(aa_indices), torch.Tensor(mod_features)` to tell the base class how to get the input features from the dataframe.\n",
    "- Re-implement `def _get_targets_from_batch_df(self, batch_df): return torch.Tensor(batch_df['rt'].values)` to tell the base class how to get the target values from dataframe for training.\n",
    "- Re-implement `def _prepare_predict_data_df(self, precursor_df): self._predict_column_in_df = 'rt_pred'...` to initialize the column to store predicted values.\n",
    "- [Optional] Re-implement `def _init_for_train(self): self.loss_func=...` to define the loss function. Defaults to L1Loss()\n",
    "- At last, execute the model in python or a notebook:\n",
    "```\n",
    "model = RTPredictionModel()\n",
    "model.build(model_class=TorchRTPrediction)\n",
    "model.train(df)\n",
    "pred_df = model.predict(df)\n",
    "```\n",
    "\n",
    "Check `peptdeep.model.rt.AlphaRTModel` for details. And `peptdeep.model.ccs.AlphaCCSModel` is also similar. MS2 prediction model is more complicated as the output value for a peptide is not a scalar, see `peptdeep.model.ms2.pDeepModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PeptideModelInterfaceBase(object):\n",
    "    def __init__(self,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model:torch.nn.Module = None\n",
    "        if 'GPU' in kwargs:\n",
    "            self.use_GPU(kwargs['GPU'])\n",
    "        else:\n",
    "            self.use_GPU(True)\n",
    "        self.optimizer = None\n",
    "\n",
    "    def use_GPU(self, GPU=True):\n",
    "        if not torch.cuda.is_available():\n",
    "            GPU=False\n",
    "        self.device = torch.device('cuda' if GPU else 'cpu')\n",
    "        if self.model is not None:\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def build(self,\n",
    "        model_class: torch.nn.Module,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model = model_class(**kwargs)\n",
    "        self.model_params = kwargs\n",
    "        self.model.to(self.device)\n",
    "        self._init_for_train()\n",
    "\n",
    "    def train_with_warmup(self,\n",
    "        precursor_df: pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024, \n",
    "        epoch=10, \n",
    "        warmup_epoch=5,\n",
    "        lr=1e-4,\n",
    "        verbose=False,\n",
    "        verbose_each_epoch=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self._pre_training(precursor_df, lr, **kwargs)\n",
    "\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer, warmup_epoch, epoch\n",
    "        )\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            batch_cost = self._train_one_epoch(\n",
    "                precursor_df, epoch,\n",
    "                batch_size, verbose_each_epoch,\n",
    "                **kwargs\n",
    "            )\n",
    "            lr_scheduler.step()\n",
    "            if verbose: print(\n",
    "                f'[Training] Epoch={epoch+1}, lr={lr_scheduler.get_last_lr()[0]}, loss={np.mean(batch_cost)}'\n",
    "            )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self,\n",
    "        precursor_df: pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024, \n",
    "        epoch=10, \n",
    "        lr=1e-4,\n",
    "        verbose=False,\n",
    "        verbose_each_epoch=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self._pre_training(precursor_df, lr, **kwargs)\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            batch_cost = self._train_one_epoch(\n",
    "                precursor_df, epoch,\n",
    "                batch_size, verbose_each_epoch,\n",
    "                **kwargs\n",
    "            )\n",
    "            if verbose: print(f'[Training] Epoch={epoch+1}, Mean Loss={np.mean(batch_cost)}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def predict(self,\n",
    "        precursor_df:pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024,\n",
    "        verbose=False,\n",
    "        **kwargs\n",
    "    )->pd.DataFrame:\n",
    "        if 'nAA' not in precursor_df.columns:\n",
    "            precursor_df['nAA'] = precursor_df.sequence.str.len()\n",
    "            precursor_df.sort_values('nAA', inplace=True)\n",
    "            precursor_df.reset_index(drop=True,inplace=True)\n",
    "        self._check_predict_in_order(precursor_df)\n",
    "        self._prepare_predict_data_df(precursor_df,**kwargs)\n",
    "        self.model.eval()\n",
    "\n",
    "        _grouped = precursor_df.groupby('nAA')\n",
    "        if verbose:\n",
    "            batch_tqdm = tqdm(_grouped)\n",
    "        else:\n",
    "            batch_tqdm = _grouped\n",
    "        with torch.no_grad():\n",
    "            for nAA, df_group in batch_tqdm:\n",
    "                for i in range(0, len(df_group), batch_size):\n",
    "                    batch_end = i+batch_size\n",
    "                    \n",
    "                    batch_df = df_group.iloc[i:batch_end,:]\n",
    "\n",
    "                    features = self._get_features_from_batch_df(\n",
    "                        batch_df, nAA, **kwargs\n",
    "                    )\n",
    "\n",
    "                    predicts = self._predict_one_batch(*features)\n",
    "\n",
    "                    self._set_batch_predict_data(\n",
    "                        batch_df, predicts, \n",
    "                        **kwargs\n",
    "                    )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.predict_df\n",
    "\n",
    "    def save(self, save_as):\n",
    "        dir = os.path.dirname(save_as)\n",
    "        if not dir: dir = './'\n",
    "        if not os.path.exists(dir): os.makedirs(dir)\n",
    "        torch.save(self.model.state_dict(), save_as)\n",
    "        with open(save_as+'.txt','w') as f: f.write(str(self.model))\n",
    "        save_yaml(save_as+'.model_const.yaml', model_const)\n",
    "        self._save_codes(save_as+'.model.py')\n",
    "        save_yaml(save_as+'.param.yaml', self.model_params)\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        model_file: Tuple[str, IO],\n",
    "        model_path_in_zip: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if isinstance(model_file, str):\n",
    "            # We may release all models (msms, rt, ccs, ...) in a single zip file\n",
    "            if model_file.lower().endswith('.zip'):\n",
    "                with ZipFile(model_file) as model_zip:\n",
    "                    with model_zip.open(model_path_in_zip,'r') as pt_file:\n",
    "                        self._load_model_file(pt_file)\n",
    "            else:\n",
    "                with open(model_file,'rb') as pt_file:\n",
    "                    self._load_model_file(pt_file)\n",
    "        else:\n",
    "            self._load_model_file(model_file)\n",
    "\n",
    "    def get_parameter_num(self):\n",
    "        return np.sum([p.numel() for p in self.model.parameters()])\n",
    "\n",
    "    def build_from_py_codes(self,\n",
    "        model_code_file:str,\n",
    "        code_file_in_zip:str=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if model_code_file.lower().endswith('.zip'):\n",
    "            with ZipFile(model_code_file, 'r') as model_zip:\n",
    "                with model_zip.open(code_file_in_zip) as f:\n",
    "                    codes = f.read()\n",
    "        else:\n",
    "            with open(model_code_file, 'r') as f:\n",
    "                codes = f.read()\n",
    "        codes = compile(\n",
    "            codes, \n",
    "            filename='model_file_py',\n",
    "            mode='exec'\n",
    "        )\n",
    "        exec(codes) #codes must contains torch model codes 'class Model(...'\n",
    "        self.model = Model(**kwargs)\n",
    "        self.model_params = kwargs\n",
    "        self.model.to(self.device)\n",
    "        self._init_for_train()\n",
    "\n",
    "    def _init_for_train(self):\n",
    "        self.loss_func = torch.nn.L1Loss()\n",
    "\n",
    "    def _load_model_file(self, stream):\n",
    "        (\n",
    "            missing_keys, unexpect_keys \n",
    "        ) = self.model.load_state_dict(torch.load(\n",
    "            stream, map_location=self.device),\n",
    "            strict=False\n",
    "        )\n",
    "        if len(missing_keys) > 0:\n",
    "            logging.warn(f\"nn parameters {missing_keys} are MISSING while loading models in {self.__class__}\")\n",
    "        if len(unexpect_keys) > 0:\n",
    "            logging.warn(f\"nn parameters {unexpect_keys} are UNEXPECTED while loading models in {self.__class__}\")\n",
    "\n",
    "    def _save_codes(self, save_as):\n",
    "        import inspect\n",
    "        code = '''import torch\\nimport peptdeep.model.base as model_base\\n'''\n",
    "        class_code = inspect.getsource(self.model.__class__)\n",
    "        code += 'class Model' + class_code[class_code.find('('):]\n",
    "        with open(save_as, 'w') as f:\n",
    "            f.write(code)\n",
    "\n",
    "    def _train_one_epoch(self, \n",
    "        precursor_df, epoch, batch_size, verbose_each_epoch, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Training for an epoch\"\"\"\n",
    "        batch_cost = []\n",
    "        _grouped = list(precursor_df.sample(frac=1).groupby('nAA'))\n",
    "        rnd_nAA = np.random.permutation(len(_grouped))\n",
    "        if verbose_each_epoch:\n",
    "            batch_tqdm = tqdm(rnd_nAA)\n",
    "        else:\n",
    "            batch_tqdm = rnd_nAA\n",
    "        for i_group in batch_tqdm:\n",
    "            nAA, df_group = _grouped[i_group]\n",
    "            # df_group = df_group.reset_index(drop=True)\n",
    "            for i in range(0, len(df_group), batch_size):\n",
    "                batch_end = i+batch_size\n",
    "\n",
    "                batch_df = df_group.iloc[i:batch_end,:]\n",
    "                targets = self._get_targets_from_batch_df(\n",
    "                    batch_df,nAA=nAA,**kwargs\n",
    "                )\n",
    "                features = self._get_features_from_batch_df(\n",
    "                    batch_df,nAA=nAA,**kwargs\n",
    "                )\n",
    "                \n",
    "                batch_cost.append(\n",
    "                    self._train_one_batch(targets, *features)\n",
    "                )\n",
    "                \n",
    "            if verbose_each_epoch:\n",
    "                batch_tqdm.set_description(\n",
    "                    f'Epoch={epoch+1}, nAA={nAA}, batch={len(batch_cost)}, loss={batch_cost[-1]:.4f}'\n",
    "                )\n",
    "        return batch_cost\n",
    "\n",
    "    def _train_one_batch(\n",
    "        self, \n",
    "        targets:torch.Tensor, \n",
    "        *features,\n",
    "    ):\n",
    "        \"\"\"Training for a mini batch\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        cost = self.loss_func(predicts, targets.to(self.device))\n",
    "        cost.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        return cost.item()\n",
    "\n",
    "    def _predict_one_batch(self,\n",
    "        *features\n",
    "    ):\n",
    "        \"\"\"Predicting for a mini batch\"\"\"\n",
    "        return self.model(\n",
    "            *[fea.to(self.device) for fea in features]\n",
    "        ).cpu().detach().numpy()\n",
    "\n",
    "    def _get_targets_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        nAA:int=None, **kwargs,\n",
    "    )->torch.Tensor:\n",
    "        \"\"\"Tell the `train()` method how to get target values from the `batch_df`.\n",
    "           All sub-classes must re-implement this method.\n",
    "\n",
    "        Args:\n",
    "            batch_df (pd.DataFrame): Dataframe of each mini batch.\n",
    "            nAA (int, optional): Peptide length. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: 'Must implement _get_targets_from_batch_df() method'\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Target value tensor\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_targets_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _get_features_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        nAA:int, **kwargs,\n",
    "    )->Tuple[torch.Tensor]:\n",
    "        \"\"\"Tell `train()` and `predict()` methods how to get feature tensors from the `batch_df`.\n",
    "           All sub-classes must re-implement this method.\n",
    "\n",
    "        Args:\n",
    "            batch_df (pd.DataFrame): Dataframe of each mini batch.\n",
    "            nAA (int, optional): Peptide length. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: 'Must implement _get_features_from_batch_df() method'\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: A feature tensor or multiple feature tensors.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_features_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _prepare_predict_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kwargs\n",
    "    ):\n",
    "        '''\n",
    "        This method must define `self._predict_column_in_df` and create a `self.predict_df` dataframe.\n",
    "        All sub-classes must re-implement this method.\n",
    "        For example for RT prediction:\n",
    "        >>> self._predict_column_in_df = 'rt_pred'\n",
    "        >>> precursor_df[self._predict_column_in_df] = 0\n",
    "        >>> self.predict_df = precursor_df\n",
    "        ...\n",
    "        '''\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _prepare_predict_data_df() method'\n",
    "        )\n",
    "\n",
    "    def _prepare_train_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kwargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def _set_batch_predict_data(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        predict_values:np.array,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Set predicted values into `self.predict_df`.\n",
    "\n",
    "        Args:\n",
    "            batch_df (pd.DataFrame): Dataframe of mini batch when predicting\n",
    "            predict_values (np.array): Predicted values\n",
    "        \"\"\"\n",
    "        predict_values[predict_values<0] = 0.0\n",
    "        if self._predict_in_order:\n",
    "            self.predict_df.loc[:,self._predict_column_in_df].values[\n",
    "                batch_df.index.values[0]:batch_df.index.values[-1]+1\n",
    "            ] = predict_values\n",
    "        else:\n",
    "            self.predict_df.loc[\n",
    "                batch_df.index,self._predict_column_in_df\n",
    "            ] = predict_values\n",
    "\n",
    "    def _init_optimizer(self, lr):\n",
    "        \"\"\"Set optimizer\"\"\"\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=lr\n",
    "        )\n",
    "\n",
    "    def set_lr(self, lr:float):\n",
    "        \"\"\"Set learning rate\"\"\"\n",
    "        if self.optimizer is None:\n",
    "            self._init_optimizer(lr)\n",
    "        else:\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "\n",
    "    def _pre_training(self, precursor_df, lr, **kwargs):\n",
    "        if 'nAA' not in precursor_df.columns:\n",
    "            precursor_df['nAA'] = precursor_df.sequence.str.len()\n",
    "        self._prepare_train_data_df(precursor_df, **kwargs)\n",
    "        self.model.train()\n",
    "\n",
    "        self.set_lr(lr)\n",
    "\n",
    "    def _check_predict_in_order(self, precursor_df:pd.DataFrame):\n",
    "        if is_precursor_sorted(precursor_df):\n",
    "            self._predict_in_order = True\n",
    "        else:\n",
    "            self._predict_in_order = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-12 22:46:23> torch layers ['input_nn.mod_nn.nn.weight', 'input_nn.aa_emb.weight', 'input_nn.pos_encoder.pe', 'meta_nn.nn.weight', 'meta_nn.nn.bias', 'hidden_nn.bert.layer.0.attention.self.query.weight', 'hidden_nn.bert.layer.0.attention.self.query.bias', 'hidden_nn.bert.layer.0.attention.self.key.weight', 'hidden_nn.bert.layer.0.attention.self.key.bias', 'hidden_nn.bert.layer.0.attention.self.value.weight', 'hidden_nn.bert.layer.0.attention.self.value.bias', 'hidden_nn.bert.layer.0.attention.output.dense.weight', 'hidden_nn.bert.layer.0.attention.output.dense.bias', 'hidden_nn.bert.layer.0.attention.output.LayerNorm.weight', 'hidden_nn.bert.layer.0.attention.output.LayerNorm.bias', 'hidden_nn.bert.layer.0.intermediate.dense.weight', 'hidden_nn.bert.layer.0.intermediate.dense.bias', 'hidden_nn.bert.layer.0.output.dense.weight', 'hidden_nn.bert.layer.0.output.dense.bias', 'hidden_nn.bert.layer.0.output.LayerNorm.weight', 'hidden_nn.bert.layer.0.output.LayerNorm.bias', 'hidden_nn.bert.layer.1.attention.self.query.weight', 'hidden_nn.bert.layer.1.attention.self.query.bias', 'hidden_nn.bert.layer.1.attention.self.key.weight', 'hidden_nn.bert.layer.1.attention.self.key.bias', 'hidden_nn.bert.layer.1.attention.self.value.weight', 'hidden_nn.bert.layer.1.attention.self.value.bias', 'hidden_nn.bert.layer.1.attention.output.dense.weight', 'hidden_nn.bert.layer.1.attention.output.dense.bias', 'hidden_nn.bert.layer.1.attention.output.LayerNorm.weight', 'hidden_nn.bert.layer.1.attention.output.LayerNorm.bias', 'hidden_nn.bert.layer.1.intermediate.dense.weight', 'hidden_nn.bert.layer.1.intermediate.dense.bias', 'hidden_nn.bert.layer.1.output.dense.weight', 'hidden_nn.bert.layer.1.output.dense.bias', 'hidden_nn.bert.layer.1.output.LayerNorm.weight', 'hidden_nn.bert.layer.1.output.LayerNorm.bias', 'hidden_nn.bert.layer.2.attention.self.query.weight', 'hidden_nn.bert.layer.2.attention.self.query.bias', 'hidden_nn.bert.layer.2.attention.self.key.weight', 'hidden_nn.bert.layer.2.attention.self.key.bias', 'hidden_nn.bert.layer.2.attention.self.value.weight', 'hidden_nn.bert.layer.2.attention.self.value.bias', 'hidden_nn.bert.layer.2.attention.output.dense.weight', 'hidden_nn.bert.layer.2.attention.output.dense.bias', 'hidden_nn.bert.layer.2.attention.output.LayerNorm.weight', 'hidden_nn.bert.layer.2.attention.output.LayerNorm.bias', 'hidden_nn.bert.layer.2.intermediate.dense.weight', 'hidden_nn.bert.layer.2.intermediate.dense.bias', 'hidden_nn.bert.layer.2.output.dense.weight', 'hidden_nn.bert.layer.2.output.dense.bias', 'hidden_nn.bert.layer.2.output.LayerNorm.weight', 'hidden_nn.bert.layer.2.output.LayerNorm.bias', 'hidden_nn.bert.layer.3.attention.self.query.weight', 'hidden_nn.bert.layer.3.attention.self.query.bias', 'hidden_nn.bert.layer.3.attention.self.key.weight', 'hidden_nn.bert.layer.3.attention.self.key.bias', 'hidden_nn.bert.layer.3.attention.self.value.weight', 'hidden_nn.bert.layer.3.attention.self.value.bias', 'hidden_nn.bert.layer.3.attention.output.dense.weight', 'hidden_nn.bert.layer.3.attention.output.dense.bias', 'hidden_nn.bert.layer.3.attention.output.LayerNorm.weight', 'hidden_nn.bert.layer.3.attention.output.LayerNorm.bias', 'hidden_nn.bert.layer.3.intermediate.dense.weight', 'hidden_nn.bert.layer.3.intermediate.dense.bias', 'hidden_nn.bert.layer.3.output.dense.weight', 'hidden_nn.bert.layer.3.output.dense.bias', 'hidden_nn.bert.layer.3.output.LayerNorm.weight', 'hidden_nn.bert.layer.3.output.LayerNorm.bias', 'output_nn.nn.0.weight', 'output_nn.nn.0.bias', 'output_nn.nn.1.weight', 'output_nn.nn.2.weight', 'output_nn.nn.2.bias', 'modloss_nn.0.bert.layer.0.attention.self.query.weight', 'modloss_nn.0.bert.layer.0.attention.self.query.bias', 'modloss_nn.0.bert.layer.0.attention.self.key.weight', 'modloss_nn.0.bert.layer.0.attention.self.key.bias', 'modloss_nn.0.bert.layer.0.attention.self.value.weight', 'modloss_nn.0.bert.layer.0.attention.self.value.bias', 'modloss_nn.0.bert.layer.0.attention.output.dense.weight', 'modloss_nn.0.bert.layer.0.attention.output.dense.bias', 'modloss_nn.0.bert.layer.0.attention.output.LayerNorm.weight', 'modloss_nn.0.bert.layer.0.attention.output.LayerNorm.bias', 'modloss_nn.0.bert.layer.0.intermediate.dense.weight', 'modloss_nn.0.bert.layer.0.intermediate.dense.bias', 'modloss_nn.0.bert.layer.0.output.dense.weight', 'modloss_nn.0.bert.layer.0.output.dense.bias', 'modloss_nn.0.bert.layer.0.output.LayerNorm.weight', 'modloss_nn.0.bert.layer.0.output.LayerNorm.bias', 'modloss_nn.1.nn.0.weight', 'modloss_nn.1.nn.0.bias', 'modloss_nn.1.nn.1.weight', 'modloss_nn.1.nn.2.weight', 'modloss_nn.1.nn.2.bias'] are missing while loading models in <class 'peptdeep.model.ms2.pDeepModel'>\n",
      "2022-04-12 22:46:23> torch layers ['rt_encoder.mod_nn.nn.weight', 'rt_encoder.input_cnn.cnn_short.weight', 'rt_encoder.input_cnn.cnn_short.bias', 'rt_encoder.input_cnn.cnn_medium.weight', 'rt_encoder.input_cnn.cnn_medium.bias', 'rt_encoder.input_cnn.cnn_long.weight', 'rt_encoder.input_cnn.cnn_long.bias', 'rt_encoder.hidden_nn.rnn_h0', 'rt_encoder.hidden_nn.rnn_c0', 'rt_encoder.hidden_nn.rnn.weight_ih_l0', 'rt_encoder.hidden_nn.rnn.weight_hh_l0', 'rt_encoder.hidden_nn.rnn.bias_ih_l0', 'rt_encoder.hidden_nn.rnn.bias_hh_l0', 'rt_encoder.hidden_nn.rnn.weight_ih_l0_reverse', 'rt_encoder.hidden_nn.rnn.weight_hh_l0_reverse', 'rt_encoder.hidden_nn.rnn.bias_ih_l0_reverse', 'rt_encoder.hidden_nn.rnn.bias_hh_l0_reverse', 'rt_encoder.hidden_nn.rnn.weight_ih_l1', 'rt_encoder.hidden_nn.rnn.weight_hh_l1', 'rt_encoder.hidden_nn.rnn.bias_ih_l1', 'rt_encoder.hidden_nn.rnn.bias_hh_l1', 'rt_encoder.hidden_nn.rnn.weight_ih_l1_reverse', 'rt_encoder.hidden_nn.rnn.weight_hh_l1_reverse', 'rt_encoder.hidden_nn.rnn.bias_ih_l1_reverse', 'rt_encoder.hidden_nn.rnn.bias_hh_l1_reverse', 'rt_encoder.attn_sum.attn.0.weight', 'rt_decoder.nn.0.weight', 'rt_decoder.nn.0.bias', 'rt_decoder.nn.1.weight', 'rt_decoder.nn.2.weight', 'rt_decoder.nn.2.bias'] are unexpected while loading models in <class 'peptdeep.model.ms2.pDeepModel'>\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from peptdeep.model.ms2 import pDeepModel\n",
    "from peptdeep.pretrained_models import model_zip\n",
    "\n",
    "pDeepModel().load(model_zip, model_path_in_zip='regular/rt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
