{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.model_interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook mainly defines the basic interface that is used to interact with the deep learning models. Its 'public' functions are intended to stay untouched over the project, while the specific workings of the interface can be changed (i.e. programming polymorphism concept). For example, models can always be loaded with the `load()` function and details of the loading can be changed by inheriting the interface and changing the functions that `load()` calls. More details are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from typing import IO, Tuple, List, Union\n",
    "from alphabase.yaml_utils import save_yaml\n",
    "from alphabase.peptide.precursor import is_precursor_sorted\n",
    "\n",
    "from peptdeep.settings import model_const\n",
    "from peptdeep.utils import logging\n",
    "\n",
    "from peptdeep.model.building_block import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# copied from huggingface\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, \n",
    "    num_training_steps, num_cycles=0.5, \n",
    "    last_epoch=-1\n",
    "):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function between 0 and `pi * cycles` after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / max(1, num_warmup_steps)\n",
    "        progress = float(\n",
    "            current_step - num_warmup_steps\n",
    "        ) / max(1, num_training_steps - num_warmup_steps)\n",
    "        return max(0.0, 0.5 * (\n",
    "            1.0 + np.cos(np.pi * num_cycles * 2.0 * progress)\n",
    "        ))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Class\n",
    "The `ModelInterface` below is intended to provide a standardized way to handle deep learning models. It does not contain the pyTorch-based models themselves, but provides methods to `load()`, `save()`, `build()`, `train()` and `predict()` new models. These methods are intended to stay unchanged. To adapt the interface to a new usecase, we inherit the interface in a new class and re-implement the relevant methods `_get_features_from_batch_df()`, `_get_targets_from_batch_df()`, `_prepare_predict_data_df()`.\n",
    "The interface will adapt the training and prediction procedures. The implementation below will automatically empty the GPU cache at the end of `train()` and `predict()` to save GPU memory.\n",
    "\n",
    "For example, if we would like to design a new model for peptides with different purposes, for example RT prediction, we need to:\n",
    "\n",
    "- Design the pytorch model (`class RTPrediction(torch.nn.Module):...`)\n",
    "- Design the sub-class inherited from PeptideModelInterfaceBase (`class RTPredictionModel(PeptideModelInterfaceBase):...`)\n",
    "- Re-implement `def _get_features_from_batch_df(self, batch_df): ... return torch.LongTensor(aa_indices), torch.Tensor(mod_features)` to tell the base class how to get the input features from the dataframe.\n",
    "- Re-implement `def _get_targets_from_batch_df(self, batch_df): return torch.Tensor(batch_df['rt'].values)` to tell the base class how to get the target values for training from the dataframe.\n",
    "- Re-implement `def _prepare_predict_data_df(self, precursor_df): self._predict_column_in_df = 'rt_pred'...` to initialize the column which will store the predicted values.\n",
    "- [Optional] Re-implement `def _set_loss_function(self): self.loss_func=...` to define the loss function. Defaults to L1Loss()\n",
    "- At last, execute the model in a python script or a notebook:\n",
    "```\n",
    "model = RTPredictionModel()\n",
    "model.build(model_class=RTPrediction)\n",
    "df = ... # the training data\n",
    "model.train(df)\n",
    "pred_df = model.predict(df)\n",
    "```\n",
    "\n",
    "Check out `peptdeep.model.rt.AlphaRTModel` for details. And `peptdeep.model.ccs.AlphaCCSModel` is also similar. MS2 prediction model is more complicated as the output value for a peptide is not a scalar, see `peptdeep.model.ms2.pDeepModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ModelInterface(object):\n",
    "    \"\"\"\n",
    "    Provides standardized methods to interact\n",
    "    with ml models. Inherit into new class and override\n",
    "    the abstract (i.e. not implemented) methods.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model:torch.nn.Module = None\n",
    "        self.optimizer = None\n",
    "        self.model_params = None\n",
    "        device_type = self._get_device_type_to_use(kwargs)\n",
    "        self.set_device(device_type)\n",
    "\n",
    "\n",
    "    def set_device(self, device_type = 'cuda'):\n",
    "        \"\"\"\n",
    "        Sets the device (e.g. gpu (cuda), cpu) to be used in the model.\n",
    "        \"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            device_type = 'cpu'\n",
    "        self.device = torch.device(device_type)\n",
    "        if self.model is not None:\n",
    "            self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def build(self,\n",
    "        model_class: torch.nn.Module,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds the model by specifying the pyTorch module, \n",
    "        the parameters, the device and the loss function.\n",
    "        \"\"\"\n",
    "        self.model = model_class(**kwargs)\n",
    "        self.model_params = kwargs\n",
    "        self.model.to(self.device)\n",
    "        self._set_loss_function()\n",
    "\n",
    "    def train_with_warmup(self,\n",
    "        precursor_df: pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024, \n",
    "        epoch=10, \n",
    "        warmup_epoch=5,\n",
    "        lr=1e-4,\n",
    "        verbose=False,\n",
    "        verbose_each_epoch=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model according to specifications. Includes a warumup \n",
    "        phase corresponds to a linear learning rate.\n",
    "        \"\"\"\n",
    "        self._pre_training(precursor_df, lr, **kwargs)\n",
    "\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer, warmup_epoch, epoch\n",
    "        )\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            batch_cost = self._train_one_epoch(\n",
    "                precursor_df, epoch,\n",
    "                batch_size, verbose_each_epoch,\n",
    "                **kwargs\n",
    "            )\n",
    "            lr_scheduler.step()\n",
    "            if verbose: print(\n",
    "                f'[Training] Epoch={epoch+1}, lr={lr_scheduler.get_last_lr()[0]}, loss={np.mean(batch_cost)}'\n",
    "            )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self,\n",
    "        precursor_df: pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024, \n",
    "        epoch=10, \n",
    "        lr=1e-4,\n",
    "        verbose=False,\n",
    "        verbose_each_epoch=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model according to specifications.\n",
    "        \"\"\"\n",
    "        self._pre_training(precursor_df, lr, **kwargs)\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            batch_cost = self._train_one_epoch(\n",
    "                precursor_df, epoch,\n",
    "                batch_size, verbose_each_epoch,\n",
    "                **kwargs\n",
    "            )\n",
    "            if verbose: print(f'[Training] Epoch={epoch+1}, Mean Loss={np.mean(batch_cost)}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def predict(self,\n",
    "        precursor_df:pd.DataFrame,\n",
    "        *,\n",
    "        batch_size=1024,\n",
    "        verbose=False,\n",
    "        **kwargs\n",
    "    )->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        The model predicts the properties based on the inputs it has been trained for.\n",
    "        Returns the ouput as a pandas dataframe.\n",
    "        \"\"\"\n",
    "        precursor_df = self._add_nAA_column_if_missing(precursor_df)\n",
    "        self._check_predict_in_order(precursor_df)\n",
    "        self._prepare_predict_data_df(precursor_df,**kwargs)\n",
    "        self.model.eval()\n",
    "\n",
    "        _grouped = precursor_df.groupby('nAA')\n",
    "        if verbose:\n",
    "            batch_tqdm = tqdm(_grouped)\n",
    "        else:\n",
    "            batch_tqdm = _grouped\n",
    "        with torch.no_grad():\n",
    "            for nAA, df_group in batch_tqdm:\n",
    "                for i in range(0, len(df_group), batch_size):\n",
    "                    batch_end = i+batch_size\n",
    "                    \n",
    "                    batch_df = df_group.iloc[i:batch_end,:]\n",
    "\n",
    "                    features = self._get_features_from_batch_df(\n",
    "                        batch_df, **kwargs\n",
    "                    )\n",
    "\n",
    "                    predicts = self._predict_one_batch(*features)\n",
    "\n",
    "                    self._set_batch_predict_data(\n",
    "                        batch_df, predicts, \n",
    "                        **kwargs\n",
    "                    )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.predict_df\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save the model state, the constants used, the code defining the model and the model parameters.\n",
    "        \"\"\"\n",
    "        # TODO save tf.keras.Model\n",
    "        dir = os.path.dirname(filename)\n",
    "        if not dir: dir = './'\n",
    "        if not os.path.exists(dir): os.makedirs(dir)\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "        with open(filename+'.txt','w') as f: f.write(str(self.model))\n",
    "        save_yaml(filename+'.model_const.yaml', model_const)\n",
    "        self._save_codes(filename+'.model.py')\n",
    "        save_yaml(filename+'.param.yaml', self.model_params)\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        model_file: Tuple[str, IO],\n",
    "        model_path_in_zip: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load a model specified in a zip file, a text file or a file stream.\n",
    "        \"\"\"\n",
    "        # TODO load tf.keras.Model\n",
    "        if isinstance(model_file, str):\n",
    "            # We may release all models (msms, rt, ccs, ...) in a single zip file\n",
    "            if model_file.lower().endswith('.zip'):\n",
    "                self._load_model_from_zipfile(model_file)\n",
    "            else:\n",
    "                self._load_model_from_textfile(model_file)\n",
    "        else:\n",
    "            self._load_model_from_filestream(model_file)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_parameter_num(self):\n",
    "        \"\"\"\n",
    "        Get total number of parameters in model.\n",
    "        \"\"\"\n",
    "        return np.sum([p.numel() for p in self.model.parameters()])\n",
    "\n",
    "    def build_from_py_codes(self,\n",
    "        model_code_file:str,\n",
    "        code_file_in_zip:str=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build the model based on a python file. Must contain a pyTorch \n",
    "        model implemented as 'class Model(...'\n",
    "        \"\"\"\n",
    "        if model_code_file.lower().endswith('.zip'):\n",
    "            with ZipFile(model_code_file, 'r') as model_zip:\n",
    "                with model_zip.open(code_file_in_zip) as f:\n",
    "                    codes = f.read()\n",
    "        else:\n",
    "            with open(model_code_file, 'r') as f:\n",
    "                codes = f.read()\n",
    "        codes = compile(\n",
    "            codes, \n",
    "            filename='model_file_py',\n",
    "            mode='exec'\n",
    "        )\n",
    "        exec(codes) #codes must contains torch model codes 'class Model(...'\n",
    "        self.model = Model(**kwargs)\n",
    "        self.model_params = kwargs\n",
    "        self.model.to(self.device)\n",
    "        self._set_loss_function()\n",
    "\n",
    "    def _set_loss_function(self):\n",
    "        self.loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "    def _load_model_from_zipfile(self, model_file):\n",
    "        with ZipFile(model_file) as model_zip:\n",
    "            with model_zip.open(model_path_in_zip,'r') as pt_file:\n",
    "                self._load_model_from_filestream(pt_file)\n",
    "\n",
    "    def _load_model_from_textfile(self, model_file):\n",
    "        with open(model_file,'rb') as pt_file:\n",
    "            self._load_model_from_filestream(pt_file)\n",
    "\n",
    "    def _load_model_from_filestream(self, stream):\n",
    "        (\n",
    "            missing_keys, unexpect_keys \n",
    "        ) = self.model.load_state_dict(torch.load(\n",
    "            stream, map_location=self.device),\n",
    "            strict=False\n",
    "        )\n",
    "        if len(missing_keys) > 0:\n",
    "            logging.warn(f\"nn parameters {missing_keys} are MISSING while loading models in {self.__class__}\")\n",
    "        if len(unexpect_keys) > 0:\n",
    "            logging.warn(f\"nn parameters {unexpect_keys} are UNEXPECTED while loading models in {self.__class__}\")\n",
    "\n",
    "    def _save_codes(self, save_as):\n",
    "        import inspect\n",
    "        code = '''import torch\\nimport peptdeep.model.base as model_base\\n'''\n",
    "        class_code = inspect.getsource(self.model.__class__)\n",
    "        code += 'class Model' + class_code[class_code.find('('):]\n",
    "        with open(save_as, 'w') as f:\n",
    "            f.write(code)\n",
    "\n",
    "    def _train_one_epoch(self, \n",
    "        precursor_df, epoch, batch_size, verbose_each_epoch, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Training for an epoch\"\"\"\n",
    "        batch_cost = []\n",
    "        _grouped = list(precursor_df.sample(frac=1).groupby('nAA'))\n",
    "        rnd_nAA = np.random.permutation(len(_grouped))\n",
    "        if verbose_each_epoch:\n",
    "            batch_tqdm = tqdm(rnd_nAA)\n",
    "        else:\n",
    "            batch_tqdm = rnd_nAA\n",
    "        for i_group in batch_tqdm:\n",
    "            nAA, df_group = _grouped[i_group]\n",
    "            # df_group = df_group.reset_index(drop=True)\n",
    "            for i in range(0, len(df_group), batch_size):\n",
    "                batch_end = i+batch_size\n",
    "\n",
    "                batch_df = df_group.iloc[i:batch_end,:]\n",
    "                targets = self._get_targets_from_batch_df(\n",
    "                    batch_df, **kwargs\n",
    "                )\n",
    "                features = self._get_features_from_batch_df(\n",
    "                    batch_df, **kwargs\n",
    "                )\n",
    "                \n",
    "                batch_cost.append(\n",
    "                    self._train_one_batch(targets, *features)\n",
    "                )\n",
    "                \n",
    "            if verbose_each_epoch:\n",
    "                batch_tqdm.set_description(\n",
    "                    f'Epoch={epoch+1}, nAA={nAA}, batch={len(batch_cost)}, loss={batch_cost[-1]:.4f}'\n",
    "                )\n",
    "        return batch_cost\n",
    "\n",
    "    def _train_one_batch(\n",
    "        self, \n",
    "        targets:torch.Tensor, \n",
    "        *features,\n",
    "    ):\n",
    "        \"\"\"Training for a mini batch\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        predicts = self.model(*[fea.to(self.device) for fea in features])\n",
    "        cost = self.loss_func(predicts, targets.to(self.device))\n",
    "        cost.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        return cost.item()\n",
    "\n",
    "    def _predict_one_batch(self,\n",
    "        *features\n",
    "    ):\n",
    "        \"\"\"Predicting for a mini batch\"\"\"\n",
    "        return self.model(\n",
    "            *[fea.to(self.device) for fea in features]\n",
    "        ).cpu().detach().numpy()\n",
    "\n",
    "    def _get_targets_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame, **kwargs,\n",
    "    )->torch.Tensor:\n",
    "        \"\"\"Tell the `train()` method how to get target values from the `batch_df`.\n",
    "           All sub-classes must re-implement this method.\n",
    "\n",
    "        Args:\n",
    "            batch_df (pd.DataFrame): Dataframe of each mini batch.\n",
    "            nAA (int, optional): Peptide length. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: 'Must implement _get_targets_from_batch_df() method'\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Target value tensor\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_targets_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _get_features_from_batch_df(self,\n",
    "        batch_df:pd.DataFrame, **kwargs,\n",
    "    )->Tuple[torch.Tensor]:\n",
    "        \"\"\"Tell `train()` and `predict()` methods how to get feature tensors from the `batch_df`.\n",
    "           All sub-classes must re-implement this method.\n",
    "\n",
    "        Args:\n",
    "            batch_df (pd.DataFrame): Dataframe of each mini batch.\n",
    "            nAA (int, optional): Peptide length. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: 'Must implement _get_features_from_batch_df() method'\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: A feature tensor or multiple feature tensors.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _get_features_from_batch_df() method'\n",
    "        )\n",
    "\n",
    "    def _prepare_predict_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This method must define `self._predict_column_in_df` and create a `self.predict_df` dataframe.\n",
    "        All sub-classes must re-implement this method.\n",
    "        \n",
    "        For example for RT prediction:\n",
    "        >>> self._predict_column_in_df = 'rt_pred'\n",
    "        >>> precursor_df[self._predict_column_in_df] = 0 (initialize the predict column in the df)\n",
    "        >>> self.predict_df = precursor_df\n",
    "        ...\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Must implement _prepare_predict_data_df() method'\n",
    "        )\n",
    "\n",
    "    def _prepare_train_data_df(self,\n",
    "        precursor_df:pd.DataFrame, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Modifications to the training dataframe can be implemented here.\n",
    "\n",
    "        Args:\n",
    "            precursor_df (pd.DataFrame): Dataframe containing the training data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _set_batch_predict_data(self,\n",
    "        batch_df:pd.DataFrame,\n",
    "        predict_values:np.array,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Set predicted values into `self.predict_df`.\n",
    "\n",
    "        Args:\n",
    "            batch_df (pd.DataFrame): Dataframe of mini batch when predicting\n",
    "            predict_values (np.array): Predicted values\n",
    "        \"\"\"\n",
    "        predict_values[predict_values<0] = 0.0\n",
    "        if self._predict_in_order:\n",
    "            self.predict_df.loc[:,self._predict_column_in_df].values[\n",
    "                batch_df.index.values[0]:batch_df.index.values[-1]+1\n",
    "            ] = predict_values\n",
    "        else:\n",
    "            self.predict_df.loc[\n",
    "                batch_df.index,self._predict_column_in_df\n",
    "            ] = predict_values\n",
    "\n",
    "    def _init_optimizer(self, lr):\n",
    "        \"\"\"Set optimizer\"\"\"\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=lr\n",
    "        )\n",
    "\n",
    "    def set_lr(self, lr:float):\n",
    "        \"\"\"Set learning rate\"\"\"\n",
    "        if self.optimizer is None:\n",
    "            self._init_optimizer(lr)\n",
    "        else:\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "\n",
    "    def _pre_training(self, precursor_df, lr, **kwargs):\n",
    "        if 'nAA' not in precursor_df.columns:\n",
    "            precursor_df['nAA'] = precursor_df.sequence.str.len()\n",
    "        self._prepare_train_data_df(precursor_df, **kwargs)\n",
    "        self.model.train()\n",
    "\n",
    "        self.set_lr(lr)\n",
    "\n",
    "    def _check_predict_in_order(self, precursor_df:pd.DataFrame):\n",
    "        if is_precursor_sorted(precursor_df):\n",
    "            self._predict_in_order = True\n",
    "        else:\n",
    "            self._predict_in_order = False\n",
    "\n",
    "    def _get_device_type_to_use(self, kwargs):\n",
    "        use_gpu = self._check_if_GPU_should_be_used(kwargs)\n",
    "        if use_gpu:\n",
    "            return 'cuda'\n",
    "        else:\n",
    "            return 'cpu'\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_if_GPU_should_be_used(kwargs):\n",
    "        if 'GPU' in kwargs:\n",
    "            return kwargs['GPU']\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_nAA_column_if_missing(precursor_df):\n",
    "        \"\"\"\n",
    "        column containing the number of Amino Acids\n",
    "        \"\"\"\n",
    "        if 'nAA' not in precursor_df.columns:\n",
    "            precursor_df['nAA'] = precursor_df.sequence.str.len()\n",
    "            precursor_df.sort_values('nAA', inplace=True)\n",
    "            precursor_df.reset_index(drop=True,inplace=True)\n",
    "        return precursor_df\n",
    "    \n",
    "    \n",
    "\n",
    "#legacy\n",
    "PeptideModelInterfaceBase = ModelInterface\n",
    "ModelInterface.use_GPU = ModelInterface.set_GPU_state\n",
    "ModelInterface._init_for_train = ModelInterface._set_loss_function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
